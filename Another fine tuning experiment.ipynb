{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c0ebc99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\r\n",
      "  Using cached datasets-2.12.0-py3-none-any.whl (474 kB)\r\n",
      "Collecting evaluate\r\n",
      "  Using cached evaluate-0.4.0-py3-none-any.whl (81 kB)\r\n",
      "Collecting transformers\r\n",
      "  Using cached transformers-4.29.0-py3-none-any.whl (7.1 MB)\r\n",
      "  Using cached transformers-4.20.1-py3-none-any.whl (4.4 MB)\r\n",
      "Collecting filelock\r\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\r\n",
      "Collecting requests\r\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\r\n",
      "Collecting pyyaml>=5.1\r\n",
      "  Using cached PyYAML-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (661 kB)\r\n",
      "Collecting numpy>=1.17\r\n",
      "  Using cached numpy-1.24.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\r\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\r\n",
      "  Using cached tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\r\n",
      "Collecting packaging>=20.0\r\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\r\n",
      "Collecting tqdm>=4.27\r\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\r\n",
      "Collecting regex!=2019.12.17\r\n",
      "  Using cached regex-2023.5.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\r\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\r\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\r\n",
      "Collecting dill<0.3.7,>=0.3.0\r\n",
      "  Using cached dill-0.3.6-py3-none-any.whl (110 kB)\r\n",
      "Collecting pandas\r\n",
      "  Using cached pandas-2.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\r\n",
      "Collecting pyarrow>=8.0.0\r\n",
      "  Using cached pyarrow-12.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\r\n",
      "Collecting responses<0.19\r\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\r\n",
      "Collecting aiohttp\r\n",
      "  Using cached aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\r\n",
      "Collecting multiprocess\r\n",
      "  Using cached multiprocess-0.70.14-py39-none-any.whl (132 kB)\r\n",
      "Collecting fsspec[http]>=2021.11.1\r\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\r\n",
      "Collecting xxhash\r\n",
      "  Using cached xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\r\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\r\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\r\n",
      "Collecting yarl<2.0,>=1.0\r\n",
      "  Using cached yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\r\n",
      "Collecting charset-normalizer<4.0,>=2.0\r\n",
      "  Using cached charset_normalizer-3.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\r\n",
      "Collecting aiosignal>=1.1.2\r\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\r\n",
      "Collecting multidict<7.0,>=4.5\r\n",
      "  Using cached multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\r\n",
      "Collecting frozenlist>=1.1.1\r\n",
      "  Using cached frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\r\n",
      "Collecting attrs>=17.3.0\r\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\r\n",
      "Collecting typing-extensions>=3.7.4.3\r\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\r\n",
      "Collecting certifi>=2017.4.17\r\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\r\n",
      "Collecting urllib3<3,>=1.21.1\r\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\r\n",
      "Collecting idna<4,>=2.5\r\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\r\n",
      "Collecting python-dateutil>=2.8.2\r\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\r\n",
      "Collecting pytz>=2020.1\r\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\r\n",
      "Collecting tzdata>=2022.1\r\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\r\n",
      "Collecting six>=1.5\r\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\r\n",
      "Installing collected packages: tokenizers, pytz, xxhash, urllib3, tzdata, typing-extensions, tqdm, six, regex, pyyaml, packaging, numpy, multidict, idna, fsspec, frozenlist, filelock, dill, charset-normalizer, certifi, attrs, async-timeout, yarl, requests, python-dateutil, pyarrow, multiprocess, aiosignal, responses, pandas, huggingface-hub, aiohttp, transformers, datasets, evaluate\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.12.1\r\n",
      "    Uninstalling tokenizers-0.12.1:\r\n",
      "      Successfully uninstalled tokenizers-0.12.1\r\n",
      "  Attempting uninstall: pytz\r\n",
      "    Found existing installation: pytz 2023.3\r\n",
      "    Uninstalling pytz-2023.3:\r\n",
      "      Successfully uninstalled pytz-2023.3\r\n",
      "  Attempting uninstall: xxhash\r\n",
      "    Found existing installation: xxhash 3.2.0\r\n",
      "    Uninstalling xxhash-3.2.0:\r\n",
      "      Successfully uninstalled xxhash-3.2.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 2.0.2\r\n",
      "    Uninstalling urllib3-2.0.2:\r\n",
      "      Successfully uninstalled urllib3-2.0.2\r\n",
      "  Attempting uninstall: tzdata\r\n",
      "    Found existing installation: tzdata 2023.3\r\n",
      "    Uninstalling tzdata-2023.3:\r\n",
      "      Successfully uninstalled tzdata-2023.3\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.5.0\r\n",
      "    Uninstalling typing_extensions-4.5.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.5.0\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.65.0\r\n",
      "    Uninstalling tqdm-4.65.0:\r\n",
      "      Successfully uninstalled tqdm-4.65.0\r\n",
      "  Attempting uninstall: six\r\n",
      "    Found existing installation: six 1.16.0\r\n",
      "    Uninstalling six-1.16.0:\r\n",
      "      Successfully uninstalled six-1.16.0\r\n",
      "  Attempting uninstall: regex\r\n",
      "    Found existing installation: regex 2023.5.5\r\n",
      "    Uninstalling regex-2023.5.5:\r\n",
      "      Successfully uninstalled regex-2023.5.5\r\n",
      "  Attempting uninstall: pyyaml\r\n",
      "    Found existing installation: PyYAML 6.0\r\n",
      "    Uninstalling PyYAML-6.0:\r\n",
      "      Successfully uninstalled PyYAML-6.0\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 23.1\r\n",
      "    Uninstalling packaging-23.1:\r\n",
      "      Successfully uninstalled packaging-23.1\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.24.3\r\n",
      "    Uninstalling numpy-1.24.3:\r\n",
      "      Successfully uninstalled numpy-1.24.3\r\n",
      "  Attempting uninstall: multidict\r\n",
      "    Found existing installation: multidict 6.0.4\r\n",
      "    Uninstalling multidict-6.0.4:\r\n",
      "      Successfully uninstalled multidict-6.0.4\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.4\r\n",
      "    Uninstalling idna-3.4:\r\n",
      "      Successfully uninstalled idna-3.4\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2023.5.0\r\n",
      "    Uninstalling fsspec-2023.5.0:\r\n",
      "      Successfully uninstalled fsspec-2023.5.0\r\n",
      "  Attempting uninstall: frozenlist\r\n",
      "    Found existing installation: frozenlist 1.3.3\r\n",
      "    Uninstalling frozenlist-1.3.3:\r\n",
      "      Successfully uninstalled frozenlist-1.3.3\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.12.0\r\n",
      "    Uninstalling filelock-3.12.0:\r\n",
      "      Successfully uninstalled filelock-3.12.0\r\n",
      "  Attempting uninstall: dill\r\n",
      "    Found existing installation: dill 0.3.6\r\n",
      "    Uninstalling dill-0.3.6:\r\n",
      "      Successfully uninstalled dill-0.3.6\r\n",
      "  Attempting uninstall: charset-normalizer\r\n",
      "    Found existing installation: charset-normalizer 3.1.0\r\n",
      "    Uninstalling charset-normalizer-3.1.0:\r\n",
      "      Successfully uninstalled charset-normalizer-3.1.0\r\n",
      "  Attempting uninstall: certifi\r\n",
      "    Found existing installation: certifi 2023.5.7\r\n",
      "    Uninstalling certifi-2023.5.7:\r\n",
      "      Successfully uninstalled certifi-2023.5.7\r\n",
      "  Attempting uninstall: attrs\r\n",
      "    Found existing installation: attrs 23.1.0\r\n",
      "    Uninstalling attrs-23.1.0:\r\n",
      "      Successfully uninstalled attrs-23.1.0\r\n",
      "  Attempting uninstall: async-timeout\r\n",
      "    Found existing installation: async-timeout 4.0.2\r\n",
      "    Uninstalling async-timeout-4.0.2:\r\n",
      "      Successfully uninstalled async-timeout-4.0.2\r\n",
      "  Attempting uninstall: yarl\r\n",
      "    Found existing installation: yarl 1.9.2\r\n",
      "    Uninstalling yarl-1.9.2:\r\n",
      "      Successfully uninstalled yarl-1.9.2\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.30.0\r\n",
      "    Uninstalling requests-2.30.0:\r\n",
      "      Successfully uninstalled requests-2.30.0\r\n",
      "  Attempting uninstall: python-dateutil\r\n",
      "    Found existing installation: python-dateutil 2.8.2\r\n",
      "    Uninstalling python-dateutil-2.8.2:\r\n",
      "      Successfully uninstalled python-dateutil-2.8.2\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 12.0.0\r\n",
      "    Uninstalling pyarrow-12.0.0:\r\n",
      "      Successfully uninstalled pyarrow-12.0.0\r\n",
      "  Attempting uninstall: multiprocess\r\n",
      "    Found existing installation: multiprocess 0.70.14\r\n",
      "    Uninstalling multiprocess-0.70.14:\r\n",
      "      Successfully uninstalled multiprocess-0.70.14\r\n",
      "  Attempting uninstall: aiosignal\r\n",
      "    Found existing installation: aiosignal 1.3.1\r\n",
      "    Uninstalling aiosignal-1.3.1:\r\n",
      "      Successfully uninstalled aiosignal-1.3.1\r\n",
      "  Attempting uninstall: responses\r\n",
      "    Found existing installation: responses 0.18.0\r\n",
      "    Uninstalling responses-0.18.0:\r\n",
      "      Successfully uninstalled responses-0.18.0\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 2.0.1\r\n",
      "    Uninstalling pandas-2.0.1:\r\n",
      "      Successfully uninstalled pandas-2.0.1\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.14.1\r\n",
      "    Uninstalling huggingface-hub-0.14.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.14.1\r\n",
      "  Attempting uninstall: aiohttp\r\n",
      "    Found existing installation: aiohttp 3.8.4\r\n",
      "    Uninstalling aiohttp-3.8.4:\r\n",
      "      Successfully uninstalled aiohttp-3.8.4\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.20.1\r\n",
      "    Uninstalling transformers-4.20.1:\r\n",
      "      Successfully uninstalled transformers-4.20.1\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.12.0\r\n",
      "    Uninstalling datasets-2.12.0:\r\n",
      "      Successfully uninstalled datasets-2.12.0\r\n",
      "  Attempting uninstall: evaluate\r\n",
      "    Found existing installation: evaluate 0.4.0\r\n",
      "    Uninstalling evaluate-0.4.0:\r\n",
      "      Successfully uninstalled evaluate-0.4.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "triton 2.0.0 requires cmake, which is not installed.\r\n",
      "triton 2.0.0 requires lit, which is not installed.\r\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-23.1.0 certifi-2023.5.7 charset-normalizer-3.1.0 datasets-2.12.0 dill-0.3.6 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 multidict-6.0.4 multiprocess-0.70.14 numpy-1.24.3 packaging-23.1 pandas-2.0.1 pyarrow-12.0.0 python-dateutil-2.8.2 pytz-2023.3 pyyaml-6.0 regex-2023.5.5 requests-2.30.0 responses-0.18.0 six-1.16.0 tokenizers-0.12.1 tqdm-4.65.0 transformers-4.20.1 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.2 xxhash-3.2.0 yarl-1.9.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets evaluate transformers --force-reinstall transformers==4.20.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9f2dd14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 21:18:48.891944: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-10 21:18:49.579340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model performance evaluation\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f35c27a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18447 entries, 0 to 668\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          18447 non-null  int64  \n",
      " 1   Дата                10873 non-null  object \n",
      " 2   Время               10873 non-null  object \n",
      " 3   Цена до             10873 non-null  float64\n",
      " 4   Цена после          10873 non-null  float64\n",
      " 5   Разница в долларах  10873 non-null  float64\n",
      " 6   Дельта в процентах  10873 non-null  float64\n",
      " 7   Текст новости       10873 non-null  object \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/amazon.csv\")\n",
    "\n",
    "for root, _, files in os.walk(\"data\"):\n",
    "    for filename in files:\n",
    "        temp_df = pd.read_csv(os.path.join(root, filename))\n",
    "        df = pd.concat([df, temp_df], axis=0, sort=False)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83010446",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                           news_text  price_change_direction\n0         Check Out What Whales Are Doing With NOC\\n                       0\n1  What 7 Analyst Ratings Have To Say About North...                       0\n2  7 Analysts Have This to Say About Northrop Gru...                       0\n3  Benzinga's Top Ratings Upgrades, Downgrades Fo...                       0\n5  Looking Into Northrop Grumman's Recent Short I...                       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_text</th>\n      <th>price_change_direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Check Out What Whales Are Doing With NOC\\n</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What 7 Analyst Ratings Have To Say About North...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7 Analysts Have This to Say About Northrop Gru...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Benzinga's Top Ratings Upgrades, Downgrades Fo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Looking Into Northrop Grumman's Recent Short I...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=[\"Текст новости\"], keep=False)\n",
    "df.drop(columns=[df.columns[0], \"Разница в долларах\", \"Дельта в процентах\"], axis=1, inplace=True)\n",
    "df.rename(columns={\"Цена до\": \"price_before\", \"Цена после\": \"price_after\", \"Дата\": \"date\", \"Время\": \"Time\", \"Текст новости\": \"news_text\"}, inplace=True)\n",
    "\n",
    "df[\"absolute_price_difference\"] = df[\"price_after\"] - df[\"price_before\"]\n",
    "df[\"percentage_price_difference\"] = df[\"absolute_price_difference\"] / df[\"price_before\"] * 100\n",
    "df[\"price_change_direction\"] = np.where(df[\"absolute_price_difference\"] > 0, 1, 0)\n",
    "df = df[[\"news_text\", \"price_change_direction\"]].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eaf0b52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 2874 records.\n",
      "The testing dataset has 54 records.\n"
     ]
    }
   ],
   "source": [
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "\n",
    "# Testing dataset\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "# Check the number of records in training and testing dataset.\n",
    "print(f'The training dataset has {len(train_data)} records.')\n",
    "print(f'The testing dataset has {len(test_data)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e035fd03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hg_train_data = Dataset.from_pandas(train_data)\n",
    "hg_test_data = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4237e38f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of hg_train_data is 2874.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'news_text': \"Will Chart Analysts Notice Bad Omen on Netflix's Chart\\n\",\n 'price_change_direction': 0,\n '__index_level_0__': 426}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The length of hg_train_data is {len(hg_train_data)}.\\n')\n",
    "\n",
    "# Check one review\n",
    "hg_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ce0c89d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "PreTrainedTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer from a pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Take a look at the tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54c897a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2874 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc2e5e75ca444b67912e41a17449fc71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/54 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b4f9f3e4f4144bdb37553e90e11261b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"news_text\"], \n",
    "                     max_length=32,\n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset_train = hg_train_data.map(tokenize_dataset)\n",
    "dataset_test = hg_test_data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac00778",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['news_text', 'price_change_direction', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2874\n",
      "})\n",
      "Dataset({\n",
      "    features: ['news_text', 'price_change_direction', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 54\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4aff089",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", problem_type=\"multi_label_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90992613",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",          \n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',            \n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,    \n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-6,\n",
    "    seed=42,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e40fc6ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 157 evaluation models in Hugging Face.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "['lvwerra/test',\n 'precision',\n 'code_eval',\n 'roc_auc',\n 'cuad',\n 'xnli',\n 'rouge',\n 'pearsonr',\n 'mse',\n 'super_glue',\n 'comet',\n 'cer',\n 'sacrebleu',\n 'mahalanobis',\n 'wer',\n 'competition_math',\n 'f1',\n 'recall',\n 'coval',\n 'mauve',\n 'xtreme_s',\n 'bleurt',\n 'ter',\n 'accuracy',\n 'exact_match',\n 'indic_glue',\n 'spearmanr',\n 'mae',\n 'squad',\n 'chrf',\n 'glue',\n 'perplexity',\n 'mean_iou',\n 'squad_v2',\n 'meteor',\n 'bleu',\n 'wiki_split',\n 'sari',\n 'frugalscore',\n 'google_bleu',\n 'bertscore',\n 'matthews_correlation',\n 'seqeval',\n 'trec_eval',\n 'rl_reliability',\n 'jordyvl/ece',\n 'angelina-wang/directional_bias_amplification',\n 'cpllab/syntaxgym',\n 'lvwerra/bary_score',\n 'kaggle/amex',\n 'kaggle/ai4code',\n 'hack/test_metric',\n 'yzha/ctc_eval',\n 'codeparrot/apps_metric',\n 'mfumanelli/geometric_mean',\n 'daiyizheng/valid',\n 'poseval',\n 'erntkn/dice_coefficient',\n 'mgfrantz/roc_auc_macro',\n 'Vlasta/pr_auc',\n 'gorkaartola/metric_for_tp_fp_samples',\n 'idsedykh/metric',\n 'idsedykh/codebleu2',\n 'idsedykh/codebleu',\n 'idsedykh/megaglue',\n 'cakiki/ndcg',\n 'brier_score',\n 'Vertaix/vendiscore',\n 'GMFTBY/dailydialogevaluate',\n 'GMFTBY/dailydialog_evaluate',\n 'jzm-mailchimp/joshs_second_test_metric',\n 'ola13/precision_at_k',\n 'yulong-me/yl_metric',\n 'abidlabs/mean_iou',\n 'abidlabs/mean_iou2',\n 'KevinSpaghetti/accuracyk',\n 'NimaBoscarino/weat',\n 'ronaldahmed/nwentfaithfulness',\n 'Viona/infolm',\n 'kyokote/my_metric2',\n 'kashif/mape',\n 'Ochiroo/rouge_mn',\n 'giulio98/code_eval_outputs',\n 'leslyarun/fbeta_score',\n 'giulio98/codebleu',\n 'anz2/iliauniiccocrevaluation',\n 'zbeloki/m2',\n 'xu1998hz/sescore',\n 'mase',\n 'mape',\n 'smape',\n 'dvitel/codebleu',\n 'NCSOFT/harim_plus',\n 'JP-SystemsX/nDCG',\n 'sportlosos/sescore',\n 'Drunper/metrica_tesi',\n 'jpxkqx/peak_signal_to_noise_ratio',\n 'jpxkqx/signal_to_reconstrution_error',\n 'hpi-dhc/FairEval',\n 'nist_mt',\n 'lvwerra/accuracy_score',\n 'character',\n 'charcut_mt',\n 'ybelkada/cocoevaluate',\n 'harshhpareek/bertscore',\n 'posicube/mean_reciprocal_rank',\n 'bstrai/classification_report',\n 'omidf/squad_precision_recall',\n 'Josh98/nl2bash_m',\n 'BucketHeadP65/confusion_matrix',\n 'BucketHeadP65/roc_curve',\n 'yonting/average_precision_score',\n 'transZ/test_parascore',\n 'transZ/sbert_cosine',\n 'hynky/sklearn_proxy',\n 'xu1998hz/sescore_english_mt',\n 'xu1998hz/sescore_german_mt',\n 'xu1998hz/sescore_english_coco',\n 'xu1998hz/sescore_english_webnlg',\n 'unnati/kendall_tau_distance',\n 'r_squared',\n 'Viona/fuzzy_reordering',\n 'Viona/kendall_tau',\n 'lhy/hamming_loss',\n 'lhy/ranking_loss',\n 'Muennighoff/code_eval',\n 'yuyijiong/quad_match_score',\n 'Splend1dchan/cosine_similarity',\n 'Yeshwant123/mcc',\n 'transformersegmentation/segmentation_scores',\n 'sma2023/wil',\n 'chanelcolgate/average_precision',\n 'ckb/unigram',\n 'Felipehonorato/eer',\n 'manueldeprada/beer',\n 'tialaeMceryu/unigram',\n 'shunzh/apps_metric',\n 'hxw15/sari_metric',\n 'mcnemar',\n 'exact_match',\n 'wilcoxon',\n 'ncoop57/levenshtein_distance',\n 'kaleidophon/almost_stochastic_order',\n 'word_length',\n 'lvwerra/element_count',\n 'word_count',\n 'text_duplicates',\n 'perplexity',\n 'label_distribution',\n 'toxicity',\n 'prb977/cooccurrence_count',\n 'regard',\n 'honest',\n 'NimaBoscarino/pseudo_perplexity',\n 'ybelkada/toxicity',\n 'ronaldahmed/ccl_win',\n 'meg/perplexity']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {len(evaluate.list_evaluation_modules())} evaluation models in Hugging Face.\\n')\n",
    "\n",
    "# List all evaluation metrics\n",
    "evaluate.list_evaluation_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2310b52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to compute the metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f60d60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: __index_level_0__, price_change_direction, news_text. If __index_level_0__, price_change_direction, news_text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2874\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 180\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 32])) must be the same as input size (torch.Size([32, 2]))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 15\u001B[0m\n\u001B[1;32m      3\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(tokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m      5\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      6\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      7\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:1409\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1406\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1407\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1408\u001B[0m )\n\u001B[0;32m-> 1409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1410\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1411\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1414\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:1651\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1649\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   1650\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1651\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1654\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1657\u001B[0m ):\n\u001B[1;32m   1658\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1659\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:2345\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2344\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2345\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2348\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/trainer.py:2377\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2376\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2377\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2378\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2379\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2380\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1594\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1592\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1593\u001B[0m         loss_fct \u001B[38;5;241m=\u001B[39m BCEWithLogitsLoss()\n\u001B[0;32m-> 1594\u001B[0m         loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1595\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict:\n\u001B[1;32m   1596\u001B[0m     output \u001B[38;5;241m=\u001B[39m (logits,) \u001B[38;5;241m+\u001B[39m outputs[\u001B[38;5;241m2\u001B[39m:]\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/loss.py:720\u001B[0m, in \u001B[0;36mBCEWithLogitsLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m    719\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 720\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbinary_cross_entropy_with_logits\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    721\u001B[0m \u001B[43m                                              \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    722\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mpos_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpos_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    723\u001B[0m \u001B[43m                                              \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:3163\u001B[0m, in \u001B[0;36mbinary_cross_entropy_with_logits\u001B[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001B[0m\n\u001B[1;32m   3160\u001B[0m     reduction_enum \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mget_enum(reduction)\n\u001B[1;32m   3162\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (target\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()):\n\u001B[0;32m-> 3163\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m) must be the same as input size (\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(target\u001B[38;5;241m.\u001B[39msize(), \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize()))\n\u001B[1;32m   3165\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mbinary_cross_entropy_with_logits(\u001B[38;5;28minput\u001B[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001B[0;31mValueError\u001B[0m: Target size (torch.Size([32, 32])) must be the same as input size (torch.Size([32, 2]))"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7e407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73072691",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d3ec5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6226b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd713ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27088c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}