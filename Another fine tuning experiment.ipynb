{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c0ebc99",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /home/alexa/.local/lib/python3.9/site-packages (4.28.1)\r\n",
      "Requirement already satisfied: datasets in /home/alexa/.local/lib/python3.9/site-packages (2.12.0)\r\n",
      "Requirement already satisfied: evaluate in /home/alexa/.local/lib/python3.9/site-packages (0.4.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (0.13.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: requests in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.28.1)\r\n",
      "Requirement already satisfied: filelock in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (3.12.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (0.14.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/alexa/.local/lib/python3.9/site-packages (from transformers) (2023.5.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (22.0)\r\n",
      "Requirement already satisfied: responses<0.19 in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (0.18.0)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (2023.5.0)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (12.0.0)\r\n",
      "Requirement already satisfied: aiohttp in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (3.8.4)\r\n",
      "Requirement already satisfied: xxhash in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (3.2.0)\r\n",
      "Requirement already satisfied: pandas in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (2.0.1)\r\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (0.3.6)\r\n",
      "Requirement already satisfied: multiprocess in /home/alexa/.local/lib/python3.9/site-packages (from datasets) (0.70.14)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alexa/.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alexa/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alexa/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/alexa/.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/alexa/.local/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alexa/.local/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9f2dd14",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Hugging Face Dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "# Model performance evaluation\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f35c27a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 18447 entries, 0 to 668\n",
      "Data columns (total 8 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Unnamed: 0          18447 non-null  int64  \n",
      " 1   Дата                10873 non-null  object \n",
      " 2   Время               10873 non-null  object \n",
      " 3   Цена до             10873 non-null  float64\n",
      " 4   Цена после          10873 non-null  float64\n",
      " 5   Разница в долларах  10873 non-null  float64\n",
      " 6   Дельта в процентах  10873 non-null  float64\n",
      " 7   Текст новости       10873 non-null  object \n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/amazon.csv\")\n",
    "\n",
    "for root, _, files in os.walk(\"data\"):\n",
    "    for filename in files:\n",
    "        temp_df = pd.read_csv(os.path.join(root, filename))\n",
    "        df = pd.concat([df, temp_df], axis=0, sort=False)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83010446",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                           news_text  price_change_direction\n0         Check Out What Whales Are Doing With NOC\\n                       0\n1  What 7 Analyst Ratings Have To Say About North...                       0\n2  7 Analysts Have This to Say About Northrop Gru...                       0\n3  Benzinga's Top Ratings Upgrades, Downgrades Fo...                       0\n5  Looking Into Northrop Grumman's Recent Short I...                       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_text</th>\n      <th>price_change_direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Check Out What Whales Are Doing With NOC\\n</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What 7 Analyst Ratings Have To Say About North...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7 Analysts Have This to Say About Northrop Gru...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Benzinga's Top Ratings Upgrades, Downgrades Fo...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Looking Into Northrop Grumman's Recent Short I...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=[\"Текст новости\"], keep=False)\n",
    "df.drop(columns=[df.columns[0], \"Разница в долларах\", \"Дельта в процентах\"], axis=1, inplace=True)\n",
    "df.rename(columns={\"Цена до\": \"price_before\", \"Цена после\": \"price_after\", \"Дата\": \"date\", \"Время\": \"Time\", \"Текст новости\": \"news_text\"}, inplace=True)\n",
    "\n",
    "df[\"absolute_price_difference\"] = df[\"price_after\"] - df[\"price_before\"]\n",
    "df[\"percentage_price_difference\"] = df[\"absolute_price_difference\"] / df[\"price_before\"] * 100\n",
    "df[\"price_change_direction\"] = np.where(df[\"absolute_price_difference\"] > 0, 1, 0)\n",
    "df = df[[\"news_text\", \"price_change_direction\"]].copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4eaf0b52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset has 2874 records.\n",
      "The testing dataset has 54 records.\n"
     ]
    }
   ],
   "source": [
    "train_data = df.sample(frac=0.8, random_state=42)\n",
    "\n",
    "# Testing dataset\n",
    "test_data = df.drop(train_data.index)\n",
    "\n",
    "# Check the number of records in training and testing dataset.\n",
    "print(f'The training dataset has {len(train_data)} records.')\n",
    "print(f'The testing dataset has {len(test_data)} records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e035fd03",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "hg_train_data = Dataset.from_pandas(train_data)\n",
    "hg_test_data = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4237e38f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of hg_train_data is 2874.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'news_text': \"Will Chart Analysts Notice Bad Omen on Netflix's Chart\\n\",\n 'price_change_direction': 0,\n '__index_level_0__': 426}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'The length of hg_train_data is {len(hg_train_data)}.\\n')\n",
    "\n",
    "# Check one review\n",
    "hg_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ce0c89d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "BertTokenizerFast(name_or_path='bert-base-cased', vocab_size=28996, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizer from a pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Take a look at the tokenizer\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b54c897a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2874 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a0a034a57264acca5890deeb2232733"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/54 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18bbda1f570141ae8ca2f422d0783699"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Funtion to tokenize data\n",
    "def tokenize_dataset(data):\n",
    "    return tokenizer(data[\"news_text\"], \n",
    "                     max_length=32, \n",
    "                     truncation=True, \n",
    "                     padding=\"max_length\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "dataset_train = hg_train_data.map(tokenize_dataset)\n",
    "dataset_test = hg_test_data.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ac00778",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['news_text', 'price_change_direction', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 2874\n",
      "})\n",
      "Dataset({\n",
      "    features: ['news_text', 'price_change_direction', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 54\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the data\n",
    "print(dataset_train)\n",
    "print(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4aff089",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8d569697e214617889f27cac7f481e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90992613",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sentiment_transfer_learning_transformer/\",          \n",
    "    logging_dir='./sentiment_transfer_learning_transformer/logs',            \n",
    "    logging_strategy='epoch',\n",
    "    logging_steps=100,    \n",
    "    num_train_epochs=2,              \n",
    "    per_device_train_batch_size=4,  \n",
    "    per_device_eval_batch_size=4,  \n",
    "    learning_rate=5e-6,\n",
    "    seed=42,\n",
    "    save_strategy='epoch',\n",
    "    save_steps=100,\n",
    "    evaluation_strategy='epoch',\n",
    "    eval_steps=100,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e40fc6ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156 evaluation models in Hugging Face.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "['lvwerra/test',\n 'precision',\n 'code_eval',\n 'roc_auc',\n 'cuad',\n 'xnli',\n 'rouge',\n 'pearsonr',\n 'mse',\n 'super_glue',\n 'comet',\n 'cer',\n 'sacrebleu',\n 'mahalanobis',\n 'wer',\n 'competition_math',\n 'f1',\n 'recall',\n 'coval',\n 'mauve',\n 'xtreme_s',\n 'bleurt',\n 'ter',\n 'accuracy',\n 'exact_match',\n 'indic_glue',\n 'spearmanr',\n 'mae',\n 'squad',\n 'chrf',\n 'glue',\n 'perplexity',\n 'mean_iou',\n 'squad_v2',\n 'meteor',\n 'bleu',\n 'wiki_split',\n 'sari',\n 'frugalscore',\n 'google_bleu',\n 'bertscore',\n 'matthews_correlation',\n 'seqeval',\n 'trec_eval',\n 'rl_reliability',\n 'jordyvl/ece',\n 'angelina-wang/directional_bias_amplification',\n 'cpllab/syntaxgym',\n 'lvwerra/bary_score',\n 'kaggle/amex',\n 'kaggle/ai4code',\n 'hack/test_metric',\n 'yzha/ctc_eval',\n 'codeparrot/apps_metric',\n 'mfumanelli/geometric_mean',\n 'daiyizheng/valid',\n 'poseval',\n 'erntkn/dice_coefficient',\n 'mgfrantz/roc_auc_macro',\n 'Vlasta/pr_auc',\n 'gorkaartola/metric_for_tp_fp_samples',\n 'idsedykh/metric',\n 'idsedykh/codebleu2',\n 'idsedykh/codebleu',\n 'idsedykh/megaglue',\n 'cakiki/ndcg',\n 'brier_score',\n 'Vertaix/vendiscore',\n 'GMFTBY/dailydialogevaluate',\n 'GMFTBY/dailydialog_evaluate',\n 'jzm-mailchimp/joshs_second_test_metric',\n 'ola13/precision_at_k',\n 'yulong-me/yl_metric',\n 'abidlabs/mean_iou',\n 'abidlabs/mean_iou2',\n 'KevinSpaghetti/accuracyk',\n 'NimaBoscarino/weat',\n 'ronaldahmed/nwentfaithfulness',\n 'Viona/infolm',\n 'kyokote/my_metric2',\n 'kashif/mape',\n 'Ochiroo/rouge_mn',\n 'giulio98/code_eval_outputs',\n 'leslyarun/fbeta_score',\n 'giulio98/codebleu',\n 'anz2/iliauniiccocrevaluation',\n 'zbeloki/m2',\n 'xu1998hz/sescore',\n 'mase',\n 'mape',\n 'smape',\n 'dvitel/codebleu',\n 'NCSOFT/harim_plus',\n 'JP-SystemsX/nDCG',\n 'sportlosos/sescore',\n 'Drunper/metrica_tesi',\n 'jpxkqx/peak_signal_to_noise_ratio',\n 'jpxkqx/signal_to_reconstrution_error',\n 'hpi-dhc/FairEval',\n 'nist_mt',\n 'lvwerra/accuracy_score',\n 'character',\n 'charcut_mt',\n 'ybelkada/cocoevaluate',\n 'harshhpareek/bertscore',\n 'posicube/mean_reciprocal_rank',\n 'bstrai/classification_report',\n 'omidf/squad_precision_recall',\n 'Josh98/nl2bash_m',\n 'BucketHeadP65/confusion_matrix',\n 'BucketHeadP65/roc_curve',\n 'yonting/average_precision_score',\n 'transZ/test_parascore',\n 'transZ/sbert_cosine',\n 'hynky/sklearn_proxy',\n 'xu1998hz/sescore_english_mt',\n 'xu1998hz/sescore_german_mt',\n 'xu1998hz/sescore_english_coco',\n 'xu1998hz/sescore_english_webnlg',\n 'unnati/kendall_tau_distance',\n 'r_squared',\n 'Viona/fuzzy_reordering',\n 'Viona/kendall_tau',\n 'lhy/hamming_loss',\n 'lhy/ranking_loss',\n 'Muennighoff/code_eval',\n 'yuyijiong/quad_match_score',\n 'Splend1dchan/cosine_similarity',\n 'Yeshwant123/mcc',\n 'transformersegmentation/segmentation_scores',\n 'sma2023/wil',\n 'chanelcolgate/average_precision',\n 'ckb/unigram',\n 'Felipehonorato/eer',\n 'manueldeprada/beer',\n 'shunzh/apps_metric',\n 'hxw15/sari_metric',\n 'mcnemar',\n 'exact_match',\n 'wilcoxon',\n 'ncoop57/levenshtein_distance',\n 'kaleidophon/almost_stochastic_order',\n 'word_length',\n 'lvwerra/element_count',\n 'word_count',\n 'text_duplicates',\n 'perplexity',\n 'label_distribution',\n 'toxicity',\n 'prb977/cooccurrence_count',\n 'regard',\n 'honest',\n 'NimaBoscarino/pseudo_perplexity',\n 'ybelkada/toxicity',\n 'ronaldahmed/ccl_win',\n 'meg/perplexity']"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'There are {len(evaluate.list_evaluation_modules())} evaluation models in Hugging Face.\\n')\n",
    "\n",
    "# List all evaluation metrics\n",
    "evaluate.list_evaluation_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2310b52",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to compute the metric\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    # probabilities = tf.nn.softmax(logits)\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7f60d60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexa/.local/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4) to match target batch_size (128).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 15\u001B[0m\n\u001B[1;32m      3\u001B[0m data_collator \u001B[38;5;241m=\u001B[39m DataCollatorForLanguageModeling(tokenizer, mlm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      6\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      7\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mdata_collator,\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1662\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1657\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_wrapped \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\n\u001B[1;32m   1659\u001B[0m inner_training_loop \u001B[38;5;241m=\u001B[39m find_executable_batch_size(\n\u001B[1;32m   1660\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inner_training_loop, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size, args\u001B[38;5;241m.\u001B[39mauto_find_batch_size\n\u001B[1;32m   1661\u001B[0m )\n\u001B[0;32m-> 1662\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:1929\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   1927\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   1928\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1929\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1931\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1932\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1933\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1934\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1935\u001B[0m ):\n\u001B[1;32m   1936\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1937\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2699\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   2696\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   2698\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 2699\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2701\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   2702\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:2731\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   2729\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   2730\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2731\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2732\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   2733\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   2734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1597\u001B[0m, in \u001B[0;36mBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1595\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msingle_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1596\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m CrossEntropyLoss()\n\u001B[0;32m-> 1597\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mloss_fct\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1598\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mproblem_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti_label_classification\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1599\u001B[0m     loss_fct \u001B[38;5;241m=\u001B[39m BCEWithLogitsLoss()\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/loss.py:1174\u001B[0m, in \u001B[0;36mCrossEntropyLoss.forward\u001B[0;34m(self, input, target)\u001B[0m\n\u001B[1;32m   1173\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, target: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m-> 1174\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1175\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreduction\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1176\u001B[0m \u001B[43m                           \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:3029\u001B[0m, in \u001B[0;36mcross_entropy\u001B[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[1;32m   3027\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m size_average \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m reduce \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3028\u001B[0m     reduction \u001B[38;5;241m=\u001B[39m _Reduction\u001B[38;5;241m.\u001B[39mlegacy_get_string(size_average, reduce)\n\u001B[0;32m-> 3029\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_entropy_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_Reduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_enum\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreduction\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_smoothing\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mValueError\u001B[0m: Expected input batch_size (4) to match target batch_size (128)."
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d7e407",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73072691",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d3ec5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6226b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd713ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27088c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}