{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --user pandas scikit-learn tensorflow --upgrade keras nltk gensim transformers evaluate numpy torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/amazon.csv\")\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for root, _, files in os.walk(\"data\"):\n",
    "    for filename in files:\n",
    "        temp_df = pd.read_csv(os.path.join(root, filename))\n",
    "        df = pd.concat([df, temp_df], axis=0, sort=False)\n",
    "\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=[\"Текст новости\"], keep=False)\n",
    "df.drop(columns=[df.columns[0], \"Разница в долларах\", \"Дельта в процентах\"], axis=1, inplace=True)\n",
    "df.rename(columns={\"Цена до\": \"price_before\", \"Цена после\": \"price_after\", \"Дата\": \"date\", \"Время\": \"Time\", \"Текст новости\": \"news_text\"}, inplace=True)\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"absolute_price_difference\"] = df[\"price_after\"] - df[\"price_before\"]\n",
    "df[\"percentage_price_difference\"] = df[\"absolute_price_difference\"] / df[\"price_before\"] * 100\n",
    "df[\"price_change_direction\"] = np.where(df[\"absolute_price_difference\"] > 0, 1, 0)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"price_change_direction\"] == 1].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[df[\"price_change_direction\"] == 0].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "news_df = pd.concat([df[\"news_text\"], df[\"price_change_direction\"]], axis=1)\n",
    "news_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = news_df[\"news_text\"].values\n",
    "y = news_df[\"price_change_direction\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size=0.2, shuffle = True, random_state=0)\n",
    "print(len(X_train),len(X_test),len(y_train),len(y_test))\n",
    "\n",
    "X_train = X_train.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,  DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_data = tokenizer(X_train, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = tf.convert_to_tensor(y_train)  # Label is already an array of 0 and 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data, labels, epochs=15, batch_size=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "acc = evaluate.load(\"accuracy\")\n",
    "# Round the predictions to turn them into \"0\" or \"1\" labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(X_test)\n",
    "predictions.predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer\n\u001B[0;32m----> 3\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m predictions \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[1;32m      5\u001B[0m predictions\u001B[38;5;241m.\u001B[39mpredictions\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/dummy_pt_objects.py:7813\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   7812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m-> 7813\u001B[0m     \u001B[43mrequires_backends\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtorch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/transformers/utils/import_utils.py:1065\u001B[0m, in \u001B[0;36mrequires_backends\u001B[0;34m(obj, backends)\u001B[0m\n\u001B[1;32m   1063\u001B[0m \u001B[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001B[39;00m\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m is_tf_available():\n\u001B[0;32m-> 1065\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001B[38;5;241m.\u001B[39mformat(name))\n\u001B[1;32m   1067\u001B[0m \u001B[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001B[39;00m\n\u001B[1;32m   1068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m backends \u001B[38;5;129;01mand\u001B[39;00m is_torch_available() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_tf_available():\n",
      "\u001B[0;31mImportError\u001B[0m: \nTrainer requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFTrainer\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(X_test)\n",
    "predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filej4k933y7.py\", line 16, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filez5q6nxif.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1651, in call  *\n            outputs = self.bert(\n        File \"/home/alexa/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_filez5q6nxif.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 774, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification):\n      • input_ids=tf.Tensor(shape=(None,), dtype=string)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test_preds \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mround(\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      2\u001B[0m test_labels \u001B[38;5;241m=\u001B[39m y_test\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTest accuracy is : \u001B[39m\u001B[38;5;124m\"\u001B[39m, acc\u001B[38;5;241m.\u001B[39mcompute(predictions \u001B[38;5;241m=\u001B[39m test_preds, references \u001B[38;5;241m=\u001B[39m test_labels))\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m/tmp/__autograph_generated_fileli9lzv6e.py:15\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001B[0;34m(iterator)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     14\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 15\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(step_function), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m), ag__\u001B[38;5;241m.\u001B[39mld(iterator)), \u001B[38;5;28;01mNone\u001B[39;00m, fscope)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     17\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/tmp/__autograph_generated_file0inq40is.py:36\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     35\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(func), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m),), \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(unpacked_inputs)), fscope)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     38\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/tmp/__autograph_generated_filej4k933y7.py:16\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, labels, training)\u001B[0m\n\u001B[1;32m     14\u001B[0m do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     15\u001B[0m retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefinedReturnValue()\n\u001B[0;32m---> 16\u001B[0m outputs \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mbert, (), \u001B[38;5;28mdict\u001B[39m(input_ids\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(input_ids), attention_mask\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(attention_mask), token_type_ids\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(token_type_ids), position_ids\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(position_ids), head_mask\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(head_mask), inputs_embeds\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(inputs_embeds), output_attentions\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(output_attentions), output_hidden_states\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(output_hidden_states), return_dict\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(return_dict), training\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(training)), fscope)\n\u001B[1;32m     17\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mld(outputs)[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m     18\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39mdropout, (), \u001B[38;5;28mdict\u001B[39m(inputs\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(pooled_output), training\u001B[38;5;241m=\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(training)), fscope)\n",
      "File \u001B[0;32m/tmp/__autograph_generated_file0inq40is.py:36\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     35\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m     retval_ \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mconverted_call(ag__\u001B[38;5;241m.\u001B[39mld(func), (ag__\u001B[38;5;241m.\u001B[39mld(\u001B[38;5;28mself\u001B[39m),), \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mag__\u001B[38;5;241m.\u001B[39mld(unpacked_inputs)), fscope)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m:\n\u001B[1;32m     38\u001B[0m     do_return \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m/tmp/__autograph_generated_filez5q6nxif.py:75\u001B[0m, in \u001B[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001B[0m\n\u001B[1;32m     73\u001B[0m input_shape \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mUndefined(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_shape\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     74\u001B[0m ag__\u001B[38;5;241m.\u001B[39mif_stmt(ag__\u001B[38;5;241m.\u001B[39mand_(\u001B[38;5;28;01mlambda\u001B[39;00m : ag__\u001B[38;5;241m.\u001B[39mld(input_ids) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mlambda\u001B[39;00m : ag__\u001B[38;5;241m.\u001B[39mld(inputs_embeds) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_shape\u001B[39m\u001B[38;5;124m'\u001B[39m,), \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 75\u001B[0m (batch_size, seq_length) \u001B[38;5;241m=\u001B[39m ag__\u001B[38;5;241m.\u001B[39mld(input_shape)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_state_4\u001B[39m():\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (past_key_values, past_key_values_length)\n",
      "\u001B[0;31mValueError\u001B[0m: in user code:\n\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"/home/alexa/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filej4k933y7.py\", line 16, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).bert, (), dict(input_ids=ag__.ld(input_ids), attention_mask=ag__.ld(attention_mask), token_type_ids=ag__.ld(token_type_ids), position_ids=ag__.ld(position_ids), head_mask=ag__.ld(head_mask), inputs_embeds=ag__.ld(inputs_embeds), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filez5q6nxif.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification).\n    \n    in user code:\n    \n        File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1651, in call  *\n            outputs = self.bert(\n        File \"/home/alexa/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_file0inq40is.py\", line 36, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_filez5q6nxif.py\", line 75, in tf__call\n            (batch_size, seq_length) = ag__.ld(input_shape)\n    \n        ValueError: Exception encountered when calling layer 'bert' (type TFBertMainLayer).\n        \n        in user code:\n        \n            File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1639, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/alexa/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 774, in call  *\n                batch_size, seq_length = input_shape\n        \n            ValueError: not enough values to unpack (expected 2, got 1)\n        \n        \n        Call arguments received by layer 'bert' (type TFBertMainLayer):\n          • input_ids=tf.Tensor(shape=(None,), dtype=string)\n          • attention_mask=None\n          • token_type_ids=None\n          • position_ids=None\n          • head_mask=None\n          • inputs_embeds=None\n          • encoder_hidden_states=None\n          • encoder_attention_mask=None\n          • past_key_values=None\n          • use_cache=None\n          • output_attentions=False\n          • output_hidden_states=False\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_bert_for_sequence_classification' (type TFBertForSequenceClassification):\n      • input_ids=tf.Tensor(shape=(None,), dtype=string)\n      • attention_mask=None\n      • token_type_ids=None\n      • position_ids=None\n      • head_mask=None\n      • inputs_embeds=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • return_dict=None\n      • labels=None\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.round(model.predict(X_test))\n",
    "test_labels = y_test\n",
    "\n",
    "print(\"Test accuracy is : \", acc.compute(predictions = test_preds, references = test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}