{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: scikit-learn in /home/alexa/.local/lib/python3.9/site-packages (1.2.2)\r\n",
      "Requirement already satisfied: tensorflow in /home/alexa/.local/lib/python3.9/site-packages (2.12.0)\r\n",
      "Requirement already satisfied: keras in /home/alexa/.local/lib/python3.9/site-packages (2.12.0)\r\n",
      "Requirement already satisfied: nltk in /home/alexa/.local/lib/python3.9/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: gensim in /home/alexa/.local/lib/python3.9/site-packages (4.3.1)\r\n",
      "Requirement already satisfied: transformers in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (4.29.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2023.3)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (1.24.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from scikit-learn) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/alexa/.local/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/alexa/.local/lib/python3.9/site-packages (from scikit-learn) (1.10.1)\r\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (2.12.3)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (2.3.0)\r\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (0.32.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (3.8.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (4.5.0)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (4.23.0)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (1.54.0)\r\n",
      "Requirement already satisfied: setuptools in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (66.0.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (2.12.0)\r\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (23.3.3)\r\n",
      "Collecting numpy>=1.20.3\r\n",
      "  Using cached numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\r\n",
      "Requirement already satisfied: packaging in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow) (23.1)\r\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (16.0.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (1.4.0)\r\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/alexa/.local/lib/python3.9/site-packages (from tensorflow) (0.4.8)\r\n",
      "Requirement already satisfied: click in /home/alexa/.local/lib/python3.9/site-packages (from nltk) (8.1.3)\r\n",
      "Requirement already satisfied: tqdm in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from nltk) (4.65.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from nltk) (2023.5.5)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/alexa/.local/lib/python3.9/site-packages (from gensim) (6.3.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.14.1)\r\n",
      "Requirement already satisfied: requests in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.30.0)\r\n",
      "Requirement already satisfied: filelock in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (3.12.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\r\n",
      "Requirement already satisfied: fsspec in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\r\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /home/alexa/.local/lib/python3.9/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\r\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/alexa/.local/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\r\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/alexa/.local/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/alexa/.local/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/alexa/.local/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.4)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/alexa/.local/lib/python3.9/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2.0.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/alexa/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/alexa/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/alexa/.local/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\r\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (4.11.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/alexa/.local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/alexa/.local/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\r\n",
      "\u001B[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages/numpy-1.24.3.dist-info/METADATA'\u001B[0m\u001B[33m\r\n",
      "\u001B[0mInstalling collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "\u001B[33m    WARNING: No metadata found in /home/alexa/miniconda3/envs/tf/lib/python3.9/site-packages\u001B[0m\u001B[33m\r\n",
      "\u001B[0m    Found existing installation: numpy 1.24.3\r\n",
      "\u001B[31mERROR: Cannot uninstall numpy 1.24.3, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps numpy==1.24.3'.\u001B[0m\u001B[31m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas scikit-learn tensorflow --upgrade keras nltk gensim transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import tensorflow as tf\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/alexa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "northrop.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-14  14:10:32  471.454987  468.269989  \\\n",
      "1  2022-10-14  11:38:17  475.109985  472.109985   \n",
      "2  2022-10-14  11:25:32  476.359985  475.109985   \n",
      "3  2022-10-14  10:37:16  476.359985  475.109985   \n",
      "4  2022-10-12  12:47:18  501.450012  501.660004   \n",
      "\n",
      "                                       Текст новости  \n",
      "0         Check Out What Whales Are Doing With NOC\\n  \n",
      "1  What 7 Analyst Ratings Have To Say About North...  \n",
      "2  7 Analysts Have This to Say About Northrop Gru...  \n",
      "3  Benzinga's Top Ratings Upgrades, Downgrades Fo...  \n",
      "4  Credit Suisse Picks 7 Aerospace, Defense Stock...  \n",
      "boeing.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-09-29  15:00:27  124.675903  124.540001  \\\n",
      "1  2022-09-28  14:30:52  133.009995  133.330002   \n",
      "2  2022-09-28  11:42:56  130.130005  130.570007   \n",
      "3  2022-09-26  14:05:53  127.320000  126.980003   \n",
      "4  2022-09-23  11:38:28  131.759995  130.229996   \n",
      "\n",
      "                                       Текст новости  \n",
      "0           Why Boeing Shares Are Getting Hammered\\n  \n",
      "1  Morgan Stanley Bets On Boeing's Prospects Desp...  \n",
      "2          Looking At Boeing's Recent Whale Trades\\n  \n",
      "3  9 Industrials Stocks With Whale Alerts In Toda...  \n",
      "4  Costco Down After Earnings as Analysts Fret Ab...  \n",
      "amd.csv          Дата     Время    Цена до  Цена после   \n",
      "0  2022-10-10  15:11:10  57.939999   57.830002  \\\n",
      "1  2022-10-10  13:36:40  57.619999   57.939999   \n",
      "2  2022-10-10  13:34:42  57.619999   57.939999   \n",
      "3  2022-10-10  12:29:53  57.494301   57.180000   \n",
      "4  2022-10-10  11:36:07  57.494301   57.180000   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  AMD Analyst Slashes Price Target By 32% On Dis...  \n",
      "1  10 Information Technology Stocks With Whale Al...  \n",
      "2                  What's Going On With AMD Shares\\n  \n",
      "3           Stocks That Hit 52-Week Lows On Monday\\n  \n",
      "4  IDC Highlights Weak PC Market, Apple Being The...  \n",
      "target.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-17  14:54:44  149.723206  149.744995  \\\n",
      "1  2022-10-17  13:11:21  149.929993  149.509995   \n",
      "2  2022-10-14  14:10:00  146.619995  146.059998   \n",
      "3  2022-10-14  13:42:51  146.619995  146.059998   \n",
      "4  2022-10-11  14:49:32  156.059998  154.279999   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  This Chipotle Analyst Sees Further Upside In 2...  \n",
      "1               Target Whale Trades For October 17\\n  \n",
      "2        This Is What Whales Are Betting On Target\\n  \n",
      "3  10 Consumer Discretionary Stocks Whale Activit...  \n",
      "4  10 Best Deals Available Now From Amazon, Targe...  \n",
      "tesla.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-17  15:23:09  218.440002  219.210007  \\\n",
      "1  2022-10-17  13:43:16  219.610001  218.440002   \n",
      "2  2022-10-17  11:22:15  217.193695  219.080002   \n",
      "3  2022-10-17  11:15:59  217.193695  219.080002   \n",
      "4  2022-10-17  10:45:26  217.193695  219.080002   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  Tesla Shares Have A Supporter in Congress: Hou...  \n",
      "1  10 Consumer Discretionary Stocks Whale Activit...  \n",
      "2  Monday's Market Minute: Three Things to Watch ...  \n",
      "3  Friday Retreat: After Wild Thursday Rally, Mar...  \n",
      "4  Elon's Mom: Stop Being Mean To My 'Genius' Son...  \n",
      "amd2.csv       Время    Цена до  Цена после   \n",
      "0  14:18:01  89.261703   89.190002  \\\n",
      "1  11:58:12  87.139999   87.949997   \n",
      "2  15:20:14  86.919998   87.330002   \n",
      "3  13:35:15  86.589996   86.309998   \n",
      "4  15:00:43  84.378899   84.315002   \n",
      "\n",
      "                                       Текст новости   \n",
      "0                    What's Going On With AMD Shares  \\\n",
      "1           What's Going On With Nvidia Shares Today   \n",
      "2  Germany's Possible Export Ban On Semiconductor...   \n",
      "3  10 Information Technology Stocks Whale Activit...   \n",
      "4  Advanced Micro Devices Unusual Options Activit...   \n",
      "\n",
      "                                          Unnamed: 5  \n",
      "0  Advanced Micro Devices, Inc. AMD shares are tr...  \n",
      "1  Nvidia Corp NVDA shares are trading higher on ...  \n",
      "2  Germany could be moving forward with a plan to...  \n",
      "3  This whale alert can help traders discover the...  \n",
      "4  Someone with a lot of money to spend has taken...  \n",
      "nike.csv          Дата     Время    Цена до  Цена после   \n",
      "0  2022-10-07  10:52:51  87.940002   87.760002  \\\n",
      "1  2022-10-06  13:38:06  90.930000   90.320000   \n",
      "2  2022-10-05  13:12:53  89.500000   90.539902   \n",
      "3  2022-10-03  14:36:10  86.220001   86.129997   \n",
      "4  2022-10-03  14:33:22  86.220001   86.129997   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  If You Invested $1,000 In Nike (NKE) Stock At ...  \n",
      "1  10 Consumer Discretionary Stocks With Whale Al...  \n",
      "2     2 NASDAQ earning reports to watch in October\\n  \n",
      "3  Tesla To Rally Around 43%? Plus This Analyst P...  \n",
      "4     Nike Stock Is Rising Today: What's Going On?\\n  \n",
      "amazon.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-21  15:21:15  118.610001  119.364998  \\\n",
      "1  2022-10-21  15:05:00  118.610001  119.364998   \n",
      "2  2022-10-21  13:55:58  117.800003  118.610001   \n",
      "3  2022-10-21  13:53:36  117.800003  118.610001   \n",
      "4  2022-10-20  14:20:03  116.440002  115.209999   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  S&P 500 Rebounds From 2022 Lows This Week As B...  \n",
      "1               What's Going On With Amazon Shares\\n  \n",
      "2  10 Consumer Discretionary Stocks Whale Activit...  \n",
      "3  Amazon To $175? Plus JP Morgan Cuts Price Targ...  \n",
      "4  Are All Streaming Services Losing Money? Netfl...  \n",
      "nvidia.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-20  13:41:21  123.739998  121.680000  \\\n",
      "1  2022-10-06  13:38:02  133.460007  132.240005   \n",
      "2  2022-10-05  14:22:50  130.951996  130.940002   \n",
      "3  2022-10-05  14:04:06  130.951996  130.940002   \n",
      "4  2022-10-05  13:55:22  130.951996  130.940002   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  10 Information Technology Stocks Whale Activit...  \n",
      "1  10 Information Technology Stocks With Whale Al...  \n",
      "2  YMTC Ban Likely A Plus For Micron & Western Di...  \n",
      "3  Why Shark Tank Investor Kevin O'Leary Plans To...  \n",
      "4  10 Information Technology Stocks Whale Activit...  \n",
      "amzn.csv       Время     Цена до  Цена после   \n",
      "0  12:00:29  104.839996  105.110603  \\\n",
      "1  14:47:48  103.379997  103.699997   \n",
      "2  11:49:10  102.620003  102.910004   \n",
      "3  15:05:42  102.500000  102.250000   \n",
      "4  13:48:31  102.885002  102.500000   \n",
      "\n",
      "                                       Текст новости   \n",
      "0  $100 Invested In Amazon.com 10 Years Ago Would...  \\\n",
      "1  Building A Stock Portfolio? Here's Your Compet...   \n",
      "2  5 Stocks To Keep An Eye On As WGA Strike Could...   \n",
      "3  Writers Strike Looms Over Hollywood: Average R...   \n",
      "4  Amazon Negates Bull Trend, Falls Under Bellwet...   \n",
      "\n",
      "                                          Unnamed: 5  \n",
      "0  Amazon.com AMZN has outperformed the market ov...  \n",
      "1  While there's no silver bullet for building a ...  \n",
      "2  Entertainment stocks were down on Tuesday afte...  \n",
      "3  Hollywood's writers are preparing to go on str...  \n",
      "4  Amazon.com, Inc AMZN was falling about 2.5% on...  \n",
      "twitter.csv          Дата     Время    Цена до  Цена после   \n",
      "0  2022-10-10  14:01:25  50.560001   50.445000  \\\n",
      "1  2022-10-07  14:02:08  49.410000   49.130001   \n",
      "2  2022-10-06  15:10:35  50.430000   50.435001   \n",
      "3  2022-10-06  13:35:35  50.720001   50.430000   \n",
      "4  2022-10-06  11:26:44  50.950100   51.040001   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  Another 'Yuge' Setback For Donald Trump's SPAC...  \n",
      "1              Twitter Whale Trades For October 07\\n  \n",
      "2  'A Suicide Bomb': NBC Reporter Details How Mus...  \n",
      "3  Selling Tesla Shares To Buy Twitter Is 'Giving...  \n",
      "4  New Book Shows Donald Trump Treated White Hous...  \n",
      "walmart.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-17  14:54:44  131.929993  131.755005  \\\n",
      "1  2022-10-14  11:32:56  131.449997  130.380005   \n",
      "2  2022-10-13  15:02:57  131.479996  132.410995   \n",
      "3  2022-10-11  14:49:32  133.619995  132.789993   \n",
      "4  2022-10-10  15:05:55  129.600006  129.649994   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  This Chipotle Analyst Sees Further Upside In 2...  \n",
      "1  These 3 REITs Could See Dividend Increases Soon\\n  \n",
      "2     Looking Into Walmart's Recent Short Interest\\n  \n",
      "3  10 Best Deals Available Now From Amazon, Targe...  \n",
      "4  These 4 Blue Chip Recession-Proof Stocks Can H...  \n",
      "apple2.csv       Время     Цена до  Цена после   \n",
      "0  15:11:18  170.250000  168.139999  \\\n",
      "1  13:35:16  170.159897  170.250000   \n",
      "2  14:47:48  168.470001  168.970001   \n",
      "3  13:35:18  168.800003  168.470001   \n",
      "4  11:49:10  167.820007  168.110001   \n",
      "\n",
      "                                       Текст новости   \n",
      "0  Taiwan Semiconductor Eyes €10B Chip Plant Debu...  \\\n",
      "1  10 Information Technology Stocks Whale Activit...   \n",
      "2  Building A Stock Portfolio? Here's Your Compet...   \n",
      "3  10 Information Technology Stocks Whale Activit...   \n",
      "4  5 Stocks To Keep An Eye On As WGA Strike Could...   \n",
      "\n",
      "                                          Unnamed: 5  \n",
      "0  Taiwan Semiconductor Manufacturing Company Ltd...  \n",
      "1  This whale alert can help traders discover the...  \n",
      "2  While there's no silver bullet for building a ...  \n",
      "3  This whale alert can help traders discover the...  \n",
      "4  Entertainment stocks were down on Tuesday afte...  \n",
      "lockheed_martin.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-17  14:32:40  398.445007  399.320007  \\\n",
      "1  2022-10-17  11:25:46  398.750000  398.350006   \n",
      "2  2022-10-17  11:15:59  398.750000  398.350006   \n",
      "3  2022-10-14  11:50:06  393.195007  391.040009   \n",
      "4  2022-10-13  14:59:49  403.839996  404.720001   \n",
      "\n",
      "                                       Текст новости  \n",
      "0       What Are Whales Doing With Lockheed Martin\\n  \n",
      "1  Solid Start: Week Begins in the Green After Ba...  \n",
      "2  Friday Retreat: After Wild Thursday Rally, Mar...  \n",
      "3  Will Chart Analysts Notice Bad Omen on Lockhee...  \n",
      "4  9 Industrials Stocks Whale Activity In Today's...  \n",
      "walt_disney.csv          Дата     Время    Цена до  Цена после   \n",
      "0  2022-10-07  13:57:47  96.959999   96.724998  \\\n",
      "1  2022-10-07  12:33:30  96.989998   96.959999   \n",
      "2  2022-10-03  13:38:15  96.510002   97.150002   \n",
      "3  2022-10-03  12:11:36  95.995003   95.820000   \n",
      "4  2022-10-03  14:16:41  96.510002   97.150002   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  Disney Is a Living Proof of a Never-Ending Cor...  \n",
      "1                 Walt Disney Whale Trades Spotted\\n  \n",
      "2  Over 20% Of The S&P 500 Is Made Up Of 5 Stocks...  \n",
      "3  Is Putin Darth Vader? Luke Skywalker Helps Ukr...  \n",
      "4  $10 Bet Could Be Worth $74,000 If This NFL Tea...  \n",
      "apple.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-21  15:21:15  146.149994  147.820099  \\\n",
      "1  2022-10-21  13:55:54  145.869995  146.149994   \n",
      "2  2022-10-21  11:57:55  144.250107  145.074997   \n",
      "3  2022-10-21  10:43:20  144.574997  144.250107   \n",
      "4  2022-10-20  13:41:21  144.285004  142.839996   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  S&P 500 Rebounds From 2022 Lows This Week As B...  \n",
      "1  10 Information Technology Stocks With Whale Al...  \n",
      "2  When Can Elon Musk Sell Tesla Shares? Could Mo...  \n",
      "3  Google's New Tweaks To Messages May Annoy iPho...  \n",
      "4  10 Information Technology Stocks Whale Activit...  \n",
      "netflix.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-20  14:20:03  270.600006  267.571106  \\\n",
      "1  2022-10-05  13:12:53  231.360001  233.850006   \n",
      "2  2022-10-03  13:36:14  238.360001  240.110001   \n",
      "3  2022-10-03  13:33:01  238.360001  240.110001   \n",
      "4  2022-10-03  11:31:58  237.690002  237.660004   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  Are All Streaming Services Losing Money? Netfl...  \n",
      "1     2 NASDAQ earning reports to watch in October\\n  \n",
      "2  10 Communication Services Stocks With Whale Al...  \n",
      "3  If You Invested $1000 In This Stock 20 Years A...  \n",
      "4                     Netflix Whale Trades Spotted\\n  \n",
      "moderna.csv          Дата     Время     Цена до  Цена после   \n",
      "0  2022-10-07  12:03:37  121.550003  121.025002  \\\n",
      "1  2022-10-03  15:19:54  121.100098  121.904999   \n",
      "2  2022-09-29  13:32:01  116.739998  116.034798   \n",
      "3  2022-09-28  13:47:14  122.440002  122.250000   \n",
      "4  2022-09-27  13:02:24  122.029999  122.050003   \n",
      "\n",
      "                                    Текст новости  \n",
      "0      Looking At Moderna's Recent Whale Trades\\n  \n",
      "1     Check Out What Whales Are Doing With MRNA\\n  \n",
      "2      Stocks That Hit 52-Week Lows On Thursday\\n  \n",
      "3    This Is What Whales Are Betting On Moderna\\n  \n",
      "4  Looking Into Moderna's Recent Short Interest\\n  \n",
      "pfizer.csv          Дата     Время    Цена до  Цена после   \n",
      "0  2022-09-29  12:24:05  44.055000   44.355000  \\\n",
      "1  2022-09-23  12:02:47  43.880001   43.654999   \n",
      "2  2022-09-23  10:41:07  44.110001   43.880001   \n",
      "3  2022-09-22  13:42:15  44.869999   44.680000   \n",
      "4  2022-09-22  10:37:42  44.540001   44.689999   \n",
      "\n",
      "                                       Текст новости  \n",
      "0  Biohaven's ALS Drug Fails To Meet Study Endpoi...  \n",
      "1  Pfizer Resumes Dosing In Phase 3 Hemophilia Ge...  \n",
      "2  Elon Musk, Jeff Bezos & Richard Branson: Which...  \n",
      "3  10 Health Care Stocks With Whale Alerts In Tod...  \n",
      "4  Pfizer To Supply 6M COVID-19 Therapy Courses T...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 21574 entries, 0 to 668\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Дата           10873 non-null  object \n",
      " 1   Время          14000 non-null  object \n",
      " 2   Цена до        14000 non-null  float64\n",
      " 3   Цена после     14000 non-null  float64\n",
      " 4   Текст новости  14000 non-null  object \n",
      " 5   Unnamed: 5     3074 non-null   object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 1.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": "           Дата     Время     Цена до  Цена после   \n0    2022-10-21  15:21:15  118.610001  119.364998  \\\n1    2022-10-21  15:05:00  118.610001  119.364998   \n2    2022-10-21  13:55:58  117.800003  118.610001   \n3    2022-10-21  13:53:36  117.800003  118.610001   \n4    2022-10-20  14:20:03  116.440002  115.209999   \n..          ...       ...         ...         ...   \n664  2020-12-10  14:47:29   41.845001   41.955002   \n665  2020-12-10  11:36:18   41.930000   41.735001   \n666  2020-12-10  11:24:03   41.939999   41.930000   \n667  2020-12-09  11:29:30   41.520000   41.779999   \n668  2020-12-09  11:10:24   41.520000   41.779999   \n\n                                         Текст новости  \n0    S&P 500 Rebounds From 2022 Lows This Week As B...  \n1                 What's Going On With Amazon Shares\\n  \n2    10 Consumer Discretionary Stocks Whale Activit...  \n3    Amazon To $175? Plus JP Morgan Cuts Price Targ...  \n4    Are All Streaming Services Losing Money? Netfl...  \n..                                                 ...  \n664  10 Health Care Stocks With Unusual Options Ale...  \n665  Rehearsal Exposes Gaps In COVID Vaccine Delive...  \n666  Investors Seem More Focused On Vaccine, Stimul...  \n667  Allergy Warning Issued For Pfizer-BioNTech COV...  \n668       Stocks That Hit 52-Week Highs On Wednesday\\n  \n\n[21574 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Дата</th>\n      <th>Время</th>\n      <th>Цена до</th>\n      <th>Цена после</th>\n      <th>Текст новости</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-10-21</td>\n      <td>15:21:15</td>\n      <td>118.610001</td>\n      <td>119.364998</td>\n      <td>S&amp;P 500 Rebounds From 2022 Lows This Week As B...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-10-21</td>\n      <td>15:05:00</td>\n      <td>118.610001</td>\n      <td>119.364998</td>\n      <td>What's Going On With Amazon Shares\\n</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-10-21</td>\n      <td>13:55:58</td>\n      <td>117.800003</td>\n      <td>118.610001</td>\n      <td>10 Consumer Discretionary Stocks Whale Activit...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-10-21</td>\n      <td>13:53:36</td>\n      <td>117.800003</td>\n      <td>118.610001</td>\n      <td>Amazon To $175? Plus JP Morgan Cuts Price Targ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-10-20</td>\n      <td>14:20:03</td>\n      <td>116.440002</td>\n      <td>115.209999</td>\n      <td>Are All Streaming Services Losing Money? Netfl...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>664</th>\n      <td>2020-12-10</td>\n      <td>14:47:29</td>\n      <td>41.845001</td>\n      <td>41.955002</td>\n      <td>10 Health Care Stocks With Unusual Options Ale...</td>\n    </tr>\n    <tr>\n      <th>665</th>\n      <td>2020-12-10</td>\n      <td>11:36:18</td>\n      <td>41.930000</td>\n      <td>41.735001</td>\n      <td>Rehearsal Exposes Gaps In COVID Vaccine Delive...</td>\n    </tr>\n    <tr>\n      <th>666</th>\n      <td>2020-12-10</td>\n      <td>11:24:03</td>\n      <td>41.939999</td>\n      <td>41.930000</td>\n      <td>Investors Seem More Focused On Vaccine, Stimul...</td>\n    </tr>\n    <tr>\n      <th>667</th>\n      <td>2020-12-09</td>\n      <td>11:29:30</td>\n      <td>41.520000</td>\n      <td>41.779999</td>\n      <td>Allergy Warning Issued For Pfizer-BioNTech COV...</td>\n    </tr>\n    <tr>\n      <th>668</th>\n      <td>2020-12-09</td>\n      <td>11:10:24</td>\n      <td>41.520000</td>\n      <td>41.779999</td>\n      <td>Stocks That Hit 52-Week Highs On Wednesday\\n</td>\n    </tr>\n  </tbody>\n</table>\n<p>21574 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/amazon.csv\", delimiter=\";\")\n",
    "df.drop(columns=[df.columns[0], \"Разница в долларах\", \"Дельта в процентах\"], axis=1, inplace=True)\n",
    "for root, _, files in os.walk(\"data\"):\n",
    "    for filename in files:\n",
    "        temp_df = pd.read_csv(os.path.join(root, filename), delimiter=\";\")\n",
    "        temp_df.drop(columns=[temp_df.columns[0], \"Разница в долларах\", \"Дельта в процентах\"], axis=1, inplace=True, errors=\"ignore\")\n",
    "        print(filename, temp_df.head())\n",
    "        df = pd.concat([df, temp_df], axis=0, sort=False)\n",
    "\n",
    "df.info()\n",
    "df.drop(columns=[df.columns[5]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           news_text  price_change_direction\n0         Check Out What Whales Are Doing With NOC\\n                      -1\n1  What 7 Analyst Ratings Have To Say About North...                      -1\n2  7 Analysts Have This to Say About Northrop Gru...                      -1\n3  Benzinga's Top Ratings Upgrades, Downgrades Fo...                      -1\n5  Looking Into Northrop Grumman's Recent Short I...                       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>news_text</th>\n      <th>price_change_direction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Check Out What Whales Are Doing With NOC\\n</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What 7 Analyst Ratings Have To Say About North...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7 Analysts Have This to Say About Northrop Gru...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Benzinga's Top Ratings Upgrades, Downgrades Fo...</td>\n      <td>-1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Looking Into Northrop Grumman's Recent Short I...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset=[\"Текст новости\"], keep=False)\n",
    "df.rename(columns={\"Цена до\": \"price_before\", \"Цена после\": \"price_after\", \"Дата\": \"date\", \"Время\": \"Time\", \"Текст новости\": \"news_text\"}, inplace=True)\n",
    "\n",
    "df[\"absolute_price_difference\"] = df[\"price_after\"] - df[\"price_before\"]\n",
    "df[\"percentage_price_difference\"] = df[\"absolute_price_difference\"] / df[\"price_before\"] * 100\n",
    "df[\"price_change_direction\"] = np.where(df[\"absolute_price_difference\"] > 0, 1, -1)\n",
    "df = df[[\"news_text\", \"price_change_direction\"]].copy()\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "news_text                 2600\nprice_change_direction    2600\ndtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"price_change_direction\"] == 1].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "news_text                 0\nprice_change_direction    0\ndtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"price_change_direction\"] == 0].count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "X = df[\"news_text\"].values\n",
    "y = df[\"price_change_direction\"].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "processed_features = []\n",
    "\n",
    "for sentence in range(len(X)):\n",
    "    processed_feature = re.sub(r'\\W', ' ', str(X[sentence]))\n",
    "\n",
    "    # remove all single characters\n",
    "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    processed_feature = processed_feature.lower()\n",
    "\n",
    "    processed_features.append(processed_feature)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['check', 'out', 'what', 'whales', 'are', 'doing', 'with', 'noc'], ['what', 'analyst', 'ratings', 'have', 'to', 'say', 'about', 'northrop', 'grumman'], ['analysts', 'have', 'this', 'to', 'say', 'about', 'northrop', 'grumman'], ['benzinga', 'top', 'ratings', 'upgrades', 'downgrades', 'for', 'october'], ['looking', 'into', 'northrop', 'grumman', 'recent', 'short', 'interest'], ['this', 'is', 'what', 'whales', 'are', 'betting', 'on', 'northrop', 'grumman'], ['here', 'how', 'much', 'invested', 'in', 'northrop', 'grumman', 'years', 'ago', 'would', 'be', 'worth', 'today'], ['uber', 'nextera', 'are', 'cnbc', 'fast', 'money', 'mentions', 'and', 'this', 'struggling', 'sector', 'is', 'tuesday', 'call', 'of', 'the', 'day'], ['what', 'analyst', 'ratings', 'have', 'to', 'say', 'about', 'northrop', 'grumman'], ['northrop', 'grumman', 'ex', 'dividend', 'date', 'is', 'friday', 'here', 'what', 'you', 'need', 'to', 'know']]\n"
     ]
    }
   ],
   "source": [
    "data_words = list(sent_to_words(processed_features))\n",
    "\n",
    "print(data_words[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['check out what whales are doing with noc', 'what analyst ratings have to say about northrop grumman', 'analysts have this to say about northrop grumman', 'benzinga top ratings upgrades downgrades for october', 'looking into northrop grumman recent short interest']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "print(data[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "labels = np.array(df['price_change_direction'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == -1:\n",
    "        y.append(0)\n",
    "    if labels[i] == 1:\n",
    "        y.append(1)\n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y, 2, dtype=\"float32\")\n",
    "del y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "5173"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  422   16 3780]\n",
      " [   0    0    0 ...   56  351  352]\n",
      " [   0    0    0 ...   56  351  352]\n",
      " ...\n",
      " [   0    0    0 ...   35   69  401]\n",
      " [   0    0    0 ...  164  915  993]\n",
      " [   0    0    0 ...   69    2  319]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[   0,    0,    0, ...,  422,   16, 3780],\n       [   0,    0,    0, ...,   56,  351,  352],\n       [   0,    0,    0, ...,   56,  351,  352],\n       ...,\n       [   0,    0,    0, ...,   35,   69,  401],\n       [   0,    0,    0, ...,  164,  915,  993],\n       [   0,    0,    0, ...,   69,    2,  319]], dtype=int32)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "news = pad_sequences(sequences, maxlen=max_len)\n",
    "print(news)\n",
    "news"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4138 1035 4138 1035\n"
     ]
    }
   ],
   "source": [
    "#  Split data to train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(news ,labels, test_size=0.2, shuffle = True, random_state=0)\n",
    "print (len(X_train),len(X_test),len(y_train),len(y_test))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Neural network without LSTM\n",
    "\n",
    "model0 = Sequential()\n",
    "\n",
    "model0.add(layers.Dense(64, activation=\"relu\"))\n",
    "model0.add(layers.Dense(32, activation=\"relu\"))\n",
    "model0.add(layers.Dropout(0.5))\n",
    "model0.add(layers.Dense(32, activation=\"relu\"))\n",
    "model0.add(layers.Dropout(0.3))\n",
    "model0.add(layers.Dense(16, activation=\"relu\"))\n",
    "model0.add(layers.Dense(2, activation=\"sigmoid\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.48986, saving model to best_model0.hdf5\n",
      "518/518 - 1s - loss: 22.9328 - accuracy: 0.5039 - val_loss: 1.5195 - val_accuracy: 0.4899 - 1s/epoch - 2ms/step\n",
      "Epoch 2/70\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.48986\n",
      "518/518 - 0s - loss: 1.0242 - accuracy: 0.4957 - val_loss: 0.7185 - val_accuracy: 0.4889 - 493ms/epoch - 951us/step\n",
      "Epoch 3/70\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.48986\n",
      "518/518 - 0s - loss: 0.7454 - accuracy: 0.4981 - val_loss: 0.7064 - val_accuracy: 0.4879 - 494ms/epoch - 954us/step\n",
      "Epoch 4/70\n",
      "\n",
      "Epoch 4: val_accuracy improved from 0.48986 to 0.51208, saving model to best_model0.hdf5\n",
      "518/518 - 1s - loss: 0.7283 - accuracy: 0.5019 - val_loss: 0.6957 - val_accuracy: 0.5121 - 551ms/epoch - 1ms/step\n",
      "Epoch 5/70\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.51208\n",
      "518/518 - 1s - loss: 0.7262 - accuracy: 0.4903 - val_loss: 0.7099 - val_accuracy: 0.4860 - 509ms/epoch - 983us/step\n",
      "Epoch 6/70\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.51208\n",
      "518/518 - 0s - loss: 0.7090 - accuracy: 0.4966 - val_loss: 0.7000 - val_accuracy: 0.4870 - 469ms/epoch - 905us/step\n",
      "Epoch 7/70\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.51208\n",
      "518/518 - 0s - loss: 0.7112 - accuracy: 0.4949 - val_loss: 0.6971 - val_accuracy: 0.4879 - 480ms/epoch - 926us/step\n",
      "Epoch 8/70\n",
      "\n",
      "Epoch 8: val_accuracy improved from 0.51208 to 0.51401, saving model to best_model0.hdf5\n",
      "518/518 - 0s - loss: 0.7007 - accuracy: 0.5036 - val_loss: 0.6970 - val_accuracy: 0.5140 - 495ms/epoch - 956us/step\n",
      "Epoch 9/70\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.51401\n",
      "518/518 - 0s - loss: 0.7097 - accuracy: 0.4911 - val_loss: 0.6974 - val_accuracy: 0.5111 - 471ms/epoch - 909us/step\n",
      "Epoch 10/70\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.51401 to 0.51498, saving model to best_model0.hdf5\n",
      "518/518 - 0s - loss: 0.7067 - accuracy: 0.4995 - val_loss: 0.6964 - val_accuracy: 0.5150 - 482ms/epoch - 930us/step\n",
      "Epoch 11/70\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.51498\n",
      "518/518 - 0s - loss: 0.7096 - accuracy: 0.4923 - val_loss: 0.7052 - val_accuracy: 0.5140 - 467ms/epoch - 902us/step\n",
      "Epoch 12/70\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.51498\n",
      "518/518 - 0s - loss: 0.7066 - accuracy: 0.4935 - val_loss: 0.7042 - val_accuracy: 0.5130 - 472ms/epoch - 911us/step\n",
      "Epoch 13/70\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.51498\n",
      "518/518 - 1s - loss: 0.7019 - accuracy: 0.5010 - val_loss: 0.7463 - val_accuracy: 0.4879 - 506ms/epoch - 977us/step\n",
      "Epoch 14/70\n",
      "\n",
      "Epoch 14: val_accuracy improved from 0.51498 to 0.51691, saving model to best_model0.hdf5\n",
      "518/518 - 0s - loss: 0.7138 - accuracy: 0.5022 - val_loss: 0.7242 - val_accuracy: 0.5169 - 493ms/epoch - 952us/step\n",
      "Epoch 15/70\n",
      "\n",
      "Epoch 15: val_accuracy improved from 0.51691 to 0.51787, saving model to best_model0.hdf5\n",
      "518/518 - 0s - loss: 0.7053 - accuracy: 0.4988 - val_loss: 0.7305 - val_accuracy: 0.5179 - 489ms/epoch - 945us/step\n",
      "Epoch 16/70\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7228 - accuracy: 0.4867 - val_loss: 0.7363 - val_accuracy: 0.5179 - 485ms/epoch - 935us/step\n",
      "Epoch 17/70\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7062 - accuracy: 0.4964 - val_loss: 0.7437 - val_accuracy: 0.4879 - 470ms/epoch - 908us/step\n",
      "Epoch 18/70\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6990 - accuracy: 0.4884 - val_loss: 0.7234 - val_accuracy: 0.5034 - 476ms/epoch - 919us/step\n",
      "Epoch 19/70\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7053 - accuracy: 0.5027 - val_loss: 0.8355 - val_accuracy: 0.4860 - 474ms/epoch - 914us/step\n",
      "Epoch 20/70\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7449 - accuracy: 0.5019 - val_loss: 0.7246 - val_accuracy: 0.4850 - 475ms/epoch - 916us/step\n",
      "Epoch 21/70\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7046 - accuracy: 0.5017 - val_loss: 0.6965 - val_accuracy: 0.5140 - 472ms/epoch - 911us/step\n",
      "Epoch 22/70\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7086 - accuracy: 0.4954 - val_loss: 0.7144 - val_accuracy: 0.4889 - 475ms/epoch - 917us/step\n",
      "Epoch 23/70\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7084 - accuracy: 0.4925 - val_loss: 0.7086 - val_accuracy: 0.5140 - 466ms/epoch - 899us/step\n",
      "Epoch 24/70\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7155 - accuracy: 0.5017 - val_loss: 0.6997 - val_accuracy: 0.5140 - 460ms/epoch - 888us/step\n",
      "Epoch 25/70\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7027 - accuracy: 0.4969 - val_loss: 0.7108 - val_accuracy: 0.5111 - 474ms/epoch - 914us/step\n",
      "Epoch 26/70\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7208 - accuracy: 0.5027 - val_loss: 0.7167 - val_accuracy: 0.4860 - 465ms/epoch - 898us/step\n",
      "Epoch 27/70\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7065 - accuracy: 0.5034 - val_loss: 0.7164 - val_accuracy: 0.4908 - 464ms/epoch - 896us/step\n",
      "Epoch 28/70\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7189 - accuracy: 0.5000 - val_loss: 0.7102 - val_accuracy: 0.5130 - 472ms/epoch - 911us/step\n",
      "Epoch 29/70\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7124 - accuracy: 0.4969 - val_loss: 0.7199 - val_accuracy: 0.4821 - 481ms/epoch - 929us/step\n",
      "Epoch 30/70\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7005 - accuracy: 0.5000 - val_loss: 0.7396 - val_accuracy: 0.4850 - 471ms/epoch - 910us/step\n",
      "Epoch 31/70\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7225 - accuracy: 0.5068 - val_loss: 0.7301 - val_accuracy: 0.4879 - 471ms/epoch - 910us/step\n",
      "Epoch 32/70\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7049 - accuracy: 0.4932 - val_loss: 0.7821 - val_accuracy: 0.4889 - 474ms/epoch - 914us/step\n",
      "Epoch 33/70\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.51787\n",
      "518/518 - 1s - loss: 0.7135 - accuracy: 0.4978 - val_loss: 0.7124 - val_accuracy: 0.4899 - 561ms/epoch - 1ms/step\n",
      "Epoch 34/70\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7028 - accuracy: 0.4959 - val_loss: 0.7295 - val_accuracy: 0.4879 - 499ms/epoch - 964us/step\n",
      "Epoch 35/70\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7112 - accuracy: 0.5068 - val_loss: 0.7251 - val_accuracy: 0.4908 - 475ms/epoch - 916us/step\n",
      "Epoch 36/70\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7126 - accuracy: 0.4998 - val_loss: 0.7323 - val_accuracy: 0.4908 - 499ms/epoch - 963us/step\n",
      "Epoch 37/70\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.51787\n",
      "518/518 - 1s - loss: 0.7004 - accuracy: 0.4935 - val_loss: 0.7750 - val_accuracy: 0.4860 - 695ms/epoch - 1ms/step\n",
      "Epoch 38/70\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6985 - accuracy: 0.4932 - val_loss: 0.7822 - val_accuracy: 0.4850 - 489ms/epoch - 945us/step\n",
      "Epoch 39/70\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7121 - accuracy: 0.4998 - val_loss: 0.7455 - val_accuracy: 0.4879 - 481ms/epoch - 929us/step\n",
      "Epoch 40/70\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6986 - accuracy: 0.4952 - val_loss: 0.7494 - val_accuracy: 0.4841 - 492ms/epoch - 950us/step\n",
      "Epoch 41/70\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7142 - accuracy: 0.4865 - val_loss: 0.7433 - val_accuracy: 0.4879 - 474ms/epoch - 916us/step\n",
      "Epoch 42/70\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6979 - accuracy: 0.4976 - val_loss: 0.8084 - val_accuracy: 0.4860 - 469ms/epoch - 906us/step\n",
      "Epoch 43/70\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7042 - accuracy: 0.4964 - val_loss: 0.7359 - val_accuracy: 0.4928 - 473ms/epoch - 914us/step\n",
      "Epoch 44/70\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.51787\n",
      "518/518 - 1s - loss: 0.7153 - accuracy: 0.5015 - val_loss: 0.7156 - val_accuracy: 0.5159 - 509ms/epoch - 983us/step\n",
      "Epoch 45/70\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.51787\n",
      "518/518 - 1s - loss: 0.7053 - accuracy: 0.4920 - val_loss: 0.7330 - val_accuracy: 0.4879 - 560ms/epoch - 1ms/step\n",
      "Epoch 46/70\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7160 - accuracy: 0.4930 - val_loss: 0.7297 - val_accuracy: 0.4879 - 483ms/epoch - 932us/step\n",
      "Epoch 47/70\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6981 - accuracy: 0.4940 - val_loss: 0.8396 - val_accuracy: 0.4908 - 477ms/epoch - 922us/step\n",
      "Epoch 48/70\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7073 - accuracy: 0.4855 - val_loss: 0.7653 - val_accuracy: 0.5111 - 474ms/epoch - 915us/step\n",
      "Epoch 49/70\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.6992 - accuracy: 0.4976 - val_loss: 0.7426 - val_accuracy: 0.5150 - 473ms/epoch - 913us/step\n",
      "Epoch 50/70\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7267 - accuracy: 0.4928 - val_loss: 0.7883 - val_accuracy: 0.4908 - 472ms/epoch - 912us/step\n",
      "Epoch 51/70\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7402 - accuracy: 0.4973 - val_loss: 0.7528 - val_accuracy: 0.4918 - 474ms/epoch - 916us/step\n",
      "Epoch 52/70\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7082 - accuracy: 0.4976 - val_loss: 0.7708 - val_accuracy: 0.5130 - 475ms/epoch - 917us/step\n",
      "Epoch 53/70\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7065 - accuracy: 0.5043 - val_loss: 0.7131 - val_accuracy: 0.5140 - 471ms/epoch - 910us/step\n",
      "Epoch 54/70\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7075 - accuracy: 0.5077 - val_loss: 0.7709 - val_accuracy: 0.4899 - 476ms/epoch - 918us/step\n",
      "Epoch 55/70\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7678 - accuracy: 0.5043 - val_loss: 0.7223 - val_accuracy: 0.5121 - 467ms/epoch - 901us/step\n",
      "Epoch 56/70\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.51787\n",
      "518/518 - 0s - loss: 0.7023 - accuracy: 0.5046 - val_loss: 0.8192 - val_accuracy: 0.5169 - 475ms/epoch - 916us/step\n",
      "Epoch 57/70\n",
      "\n",
      "Epoch 57: val_accuracy improved from 0.51787 to 0.52077, saving model to best_model0.hdf5\n",
      "518/518 - 0s - loss: 0.7245 - accuracy: 0.4937 - val_loss: 0.7181 - val_accuracy: 0.5208 - 489ms/epoch - 944us/step\n",
      "Epoch 58/70\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7025 - accuracy: 0.5034 - val_loss: 0.7185 - val_accuracy: 0.4889 - 474ms/epoch - 915us/step\n",
      "Epoch 59/70\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7226 - accuracy: 0.5043 - val_loss: 0.7386 - val_accuracy: 0.4899 - 481ms/epoch - 929us/step\n",
      "Epoch 60/70\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7036 - accuracy: 0.5012 - val_loss: 0.7321 - val_accuracy: 0.4889 - 473ms/epoch - 913us/step\n",
      "Epoch 61/70\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.6993 - accuracy: 0.4986 - val_loss: 0.8445 - val_accuracy: 0.4831 - 482ms/epoch - 931us/step\n",
      "Epoch 62/70\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7219 - accuracy: 0.5046 - val_loss: 0.7799 - val_accuracy: 0.4870 - 472ms/epoch - 910us/step\n",
      "Epoch 63/70\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7018 - accuracy: 0.5036 - val_loss: 0.7337 - val_accuracy: 0.4928 - 471ms/epoch - 909us/step\n",
      "Epoch 64/70\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7117 - accuracy: 0.5039 - val_loss: 0.7583 - val_accuracy: 0.4899 - 468ms/epoch - 903us/step\n",
      "Epoch 65/70\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.52077\n",
      "518/518 - 1s - loss: 0.6984 - accuracy: 0.5063 - val_loss: 0.7703 - val_accuracy: 0.4908 - 508ms/epoch - 981us/step\n",
      "Epoch 66/70\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7190 - accuracy: 0.4978 - val_loss: 0.7587 - val_accuracy: 0.4860 - 473ms/epoch - 913us/step\n",
      "Epoch 67/70\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7019 - accuracy: 0.5056 - val_loss: 0.7320 - val_accuracy: 0.4899 - 489ms/epoch - 945us/step\n",
      "Epoch 68/70\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7124 - accuracy: 0.5109 - val_loss: 0.9220 - val_accuracy: 0.4754 - 472ms/epoch - 911us/step\n",
      "Epoch 69/70\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7217 - accuracy: 0.4944 - val_loss: 0.7345 - val_accuracy: 0.4899 - 473ms/epoch - 913us/step\n",
      "Epoch 70/70\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.52077\n",
      "518/518 - 0s - loss: 0.7176 - accuracy: 0.4971 - val_loss: 0.7989 - val_accuracy: 0.4918 - 473ms/epoch - 914us/step\n"
     ]
    }
   ],
   "source": [
    "model0.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint0 = ModelCheckpoint(\"best_model0.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1, save_weights_only=False)\n",
    "\n",
    "history = model0.fit(X_train, y_train,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     verbose=2,\n",
    "                     epochs=70,\n",
    "                     batch_size=8,\n",
    "                     shuffle=True,\n",
    "                     callbacks=[checkpoint0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Neural network creation\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(layers.Embedding(max_words, 20))\n",
    "model1.add(layers.SpatialDropout1D(0.2))\n",
    "model1.add(layers.LSTM(30, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "model1.add(layers.Dense(2, activation='sigmoid'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56618, saving model to best_model1.hdf5\n",
      "518/518 - 38s - loss: 0.1320 - accuracy: 0.9280 - val_loss: 2.3035 - val_accuracy: 0.5662 - 38s/epoch - 73ms/step\n",
      "Epoch 2/70\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.56618 to 0.57005, saving model to best_model1.hdf5\n",
      "518/518 - 35s - loss: 0.1344 - accuracy: 0.9275 - val_loss: 2.0841 - val_accuracy: 0.5700 - 35s/epoch - 68ms/step\n",
      "Epoch 3/70\n",
      "\n",
      "Epoch 3: val_accuracy improved from 0.57005 to 0.58164, saving model to best_model1.hdf5\n",
      "518/518 - 36s - loss: 0.1271 - accuracy: 0.9345 - val_loss: 1.8439 - val_accuracy: 0.5816 - 36s/epoch - 70ms/step\n",
      "Epoch 4/70\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.58164\n",
      "518/518 - 36s - loss: 0.1350 - accuracy: 0.9241 - val_loss: 2.1416 - val_accuracy: 0.5681 - 36s/epoch - 70ms/step\n",
      "Epoch 5/70\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.58164\n",
      "518/518 - 46s - loss: 0.1294 - accuracy: 0.9309 - val_loss: 2.2479 - val_accuracy: 0.5681 - 46s/epoch - 88ms/step\n",
      "Epoch 6/70\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.58164\n",
      "518/518 - 35s - loss: 0.1198 - accuracy: 0.9297 - val_loss: 2.2599 - val_accuracy: 0.5749 - 35s/epoch - 68ms/step\n",
      "Epoch 7/70\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.58164\n",
      "518/518 - 35s - loss: 0.1213 - accuracy: 0.9321 - val_loss: 2.4760 - val_accuracy: 0.5643 - 35s/epoch - 68ms/step\n",
      "Epoch 8/70\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.58164\n",
      "518/518 - 35s - loss: 0.1238 - accuracy: 0.9282 - val_loss: 2.3327 - val_accuracy: 0.5671 - 35s/epoch - 68ms/step\n",
      "Epoch 9/70\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.58164\n",
      "518/518 - 35s - loss: 0.1152 - accuracy: 0.9323 - val_loss: 2.3945 - val_accuracy: 0.5720 - 35s/epoch - 68ms/step\n",
      "Epoch 10/70\n",
      "\n",
      "Epoch 10: val_accuracy improved from 0.58164 to 0.58261, saving model to best_model1.hdf5\n",
      "518/518 - 35s - loss: 0.1177 - accuracy: 0.9331 - val_loss: 2.4761 - val_accuracy: 0.5826 - 35s/epoch - 68ms/step\n",
      "Epoch 11/70\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1186 - accuracy: 0.9338 - val_loss: 2.3999 - val_accuracy: 0.5768 - 35s/epoch - 68ms/step\n",
      "Epoch 12/70\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1157 - accuracy: 0.9377 - val_loss: 2.4412 - val_accuracy: 0.5778 - 35s/epoch - 68ms/step\n",
      "Epoch 13/70\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1240 - accuracy: 0.9352 - val_loss: 2.5679 - val_accuracy: 0.5720 - 35s/epoch - 68ms/step\n",
      "Epoch 14/70\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1173 - accuracy: 0.9360 - val_loss: 2.4504 - val_accuracy: 0.5807 - 35s/epoch - 68ms/step\n",
      "Epoch 15/70\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1066 - accuracy: 0.9389 - val_loss: 2.6383 - val_accuracy: 0.5797 - 35s/epoch - 68ms/step\n",
      "Epoch 16/70\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.58261\n",
      "518/518 - 35s - loss: 0.1135 - accuracy: 0.9352 - val_loss: 2.6163 - val_accuracy: 0.5787 - 35s/epoch - 68ms/step\n",
      "Epoch 17/70\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.58261\n",
      "518/518 - 36s - loss: 0.1107 - accuracy: 0.9364 - val_loss: 2.5836 - val_accuracy: 0.5787 - 36s/epoch - 69ms/step\n",
      "Epoch 18/70\n",
      "\n",
      "Epoch 18: val_accuracy improved from 0.58261 to 0.58937, saving model to best_model1.hdf5\n",
      "518/518 - 38s - loss: 0.1109 - accuracy: 0.9369 - val_loss: 2.4188 - val_accuracy: 0.5894 - 38s/epoch - 74ms/step\n",
      "Epoch 19/70\n",
      "\n",
      "Epoch 19: val_accuracy improved from 0.58937 to 0.59324, saving model to best_model1.hdf5\n",
      "518/518 - 36s - loss: 0.1112 - accuracy: 0.9372 - val_loss: 2.5959 - val_accuracy: 0.5932 - 36s/epoch - 70ms/step\n",
      "Epoch 20/70\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1103 - accuracy: 0.9362 - val_loss: 2.5555 - val_accuracy: 0.5874 - 35s/epoch - 67ms/step\n",
      "Epoch 21/70\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.59324\n",
      "518/518 - 36s - loss: 0.1098 - accuracy: 0.9425 - val_loss: 2.5058 - val_accuracy: 0.5778 - 36s/epoch - 70ms/step\n",
      "Epoch 22/70\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.59324\n",
      "518/518 - 37s - loss: 0.1050 - accuracy: 0.9398 - val_loss: 2.5445 - val_accuracy: 0.5778 - 37s/epoch - 72ms/step\n",
      "Epoch 23/70\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.59324\n",
      "518/518 - 36s - loss: 0.1044 - accuracy: 0.9403 - val_loss: 2.6853 - val_accuracy: 0.5826 - 36s/epoch - 70ms/step\n",
      "Epoch 24/70\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1042 - accuracy: 0.9410 - val_loss: 2.6815 - val_accuracy: 0.5758 - 35s/epoch - 68ms/step\n",
      "Epoch 25/70\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1060 - accuracy: 0.9381 - val_loss: 2.6149 - val_accuracy: 0.5787 - 35s/epoch - 68ms/step\n",
      "Epoch 26/70\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.59324\n",
      "518/518 - 36s - loss: 0.1043 - accuracy: 0.9437 - val_loss: 2.9882 - val_accuracy: 0.5855 - 36s/epoch - 70ms/step\n",
      "Epoch 27/70\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.59324\n",
      "518/518 - 38s - loss: 0.1078 - accuracy: 0.9396 - val_loss: 2.5267 - val_accuracy: 0.5768 - 38s/epoch - 73ms/step\n",
      "Epoch 28/70\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1014 - accuracy: 0.9427 - val_loss: 2.7942 - val_accuracy: 0.5768 - 35s/epoch - 68ms/step\n",
      "Epoch 29/70\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0998 - accuracy: 0.9468 - val_loss: 2.7996 - val_accuracy: 0.5768 - 35s/epoch - 68ms/step\n",
      "Epoch 30/70\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1012 - accuracy: 0.9456 - val_loss: 2.8451 - val_accuracy: 0.5894 - 35s/epoch - 68ms/step\n",
      "Epoch 31/70\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1021 - accuracy: 0.9432 - val_loss: 2.8686 - val_accuracy: 0.5729 - 35s/epoch - 68ms/step\n",
      "Epoch 32/70\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0947 - accuracy: 0.9454 - val_loss: 3.1619 - val_accuracy: 0.5932 - 35s/epoch - 68ms/step\n",
      "Epoch 33/70\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0992 - accuracy: 0.9456 - val_loss: 2.6477 - val_accuracy: 0.5778 - 35s/epoch - 68ms/step\n",
      "Epoch 34/70\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1027 - accuracy: 0.9425 - val_loss: 2.7861 - val_accuracy: 0.5729 - 35s/epoch - 68ms/step\n",
      "Epoch 35/70\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0960 - accuracy: 0.9461 - val_loss: 2.8038 - val_accuracy: 0.5700 - 35s/epoch - 68ms/step\n",
      "Epoch 36/70\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0974 - accuracy: 0.9495 - val_loss: 2.7199 - val_accuracy: 0.5807 - 35s/epoch - 68ms/step\n",
      "Epoch 37/70\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1011 - accuracy: 0.9461 - val_loss: 2.6126 - val_accuracy: 0.5787 - 35s/epoch - 68ms/step\n",
      "Epoch 38/70\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0974 - accuracy: 0.9485 - val_loss: 2.6344 - val_accuracy: 0.5807 - 35s/epoch - 68ms/step\n",
      "Epoch 39/70\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0937 - accuracy: 0.9478 - val_loss: 2.7267 - val_accuracy: 0.5865 - 35s/epoch - 68ms/step\n",
      "Epoch 40/70\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.59324\n",
      "518/518 - 36s - loss: 0.0946 - accuracy: 0.9473 - val_loss: 2.5254 - val_accuracy: 0.5913 - 36s/epoch - 69ms/step\n",
      "Epoch 41/70\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0943 - accuracy: 0.9471 - val_loss: 2.6061 - val_accuracy: 0.5739 - 35s/epoch - 68ms/step\n",
      "Epoch 42/70\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0937 - accuracy: 0.9483 - val_loss: 2.5945 - val_accuracy: 0.5903 - 35s/epoch - 68ms/step\n",
      "Epoch 43/70\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0927 - accuracy: 0.9485 - val_loss: 2.9245 - val_accuracy: 0.5845 - 35s/epoch - 68ms/step\n",
      "Epoch 44/70\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0973 - accuracy: 0.9502 - val_loss: 2.8689 - val_accuracy: 0.5836 - 35s/epoch - 68ms/step\n",
      "Epoch 45/70\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0944 - accuracy: 0.9480 - val_loss: 2.6238 - val_accuracy: 0.5845 - 35s/epoch - 68ms/step\n",
      "Epoch 46/70\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0924 - accuracy: 0.9485 - val_loss: 2.8265 - val_accuracy: 0.5836 - 35s/epoch - 68ms/step\n",
      "Epoch 47/70\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0928 - accuracy: 0.9505 - val_loss: 2.6364 - val_accuracy: 0.5816 - 35s/epoch - 68ms/step\n",
      "Epoch 48/70\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.1003 - accuracy: 0.9454 - val_loss: 2.7634 - val_accuracy: 0.5884 - 35s/epoch - 68ms/step\n",
      "Epoch 49/70\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0905 - accuracy: 0.9471 - val_loss: 2.6350 - val_accuracy: 0.5855 - 35s/epoch - 68ms/step\n",
      "Epoch 50/70\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.59324\n",
      "518/518 - 35s - loss: 0.0943 - accuracy: 0.9468 - val_loss: 2.7520 - val_accuracy: 0.5816 - 35s/epoch - 68ms/step\n",
      "Epoch 51/70\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.59324\n",
      "518/518 - 37s - loss: 0.0902 - accuracy: 0.9517 - val_loss: 2.7597 - val_accuracy: 0.5845 - 37s/epoch - 72ms/step\n",
      "Epoch 52/70\n",
      "\n",
      "Epoch 52: val_accuracy improved from 0.59324 to 0.59517, saving model to best_model1.hdf5\n",
      "518/518 - 37s - loss: 0.0969 - accuracy: 0.9459 - val_loss: 2.5505 - val_accuracy: 0.5952 - 37s/epoch - 71ms/step\n",
      "Epoch 53/70\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.59517\n",
      "518/518 - 37s - loss: 0.0933 - accuracy: 0.9517 - val_loss: 2.8704 - val_accuracy: 0.5826 - 37s/epoch - 71ms/step\n",
      "Epoch 54/70\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.59517\n",
      "518/518 - 37s - loss: 0.0890 - accuracy: 0.9524 - val_loss: 2.9102 - val_accuracy: 0.5884 - 37s/epoch - 72ms/step\n",
      "Epoch 55/70\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.59517\n",
      "518/518 - 37s - loss: 0.0862 - accuracy: 0.9529 - val_loss: 2.9595 - val_accuracy: 0.5797 - 37s/epoch - 71ms/step\n",
      "Epoch 56/70\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.59517\n",
      "518/518 - 36s - loss: 0.0849 - accuracy: 0.9558 - val_loss: 2.8569 - val_accuracy: 0.5816 - 36s/epoch - 70ms/step\n",
      "Epoch 57/70\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.59517\n",
      "518/518 - 39s - loss: 0.0928 - accuracy: 0.9546 - val_loss: 2.9277 - val_accuracy: 0.5855 - 39s/epoch - 75ms/step\n",
      "Epoch 58/70\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.59517\n",
      "518/518 - 37s - loss: 0.0898 - accuracy: 0.9517 - val_loss: 3.0081 - val_accuracy: 0.5826 - 37s/epoch - 71ms/step\n",
      "Epoch 59/70\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.59517\n",
      "518/518 - 36s - loss: 0.0908 - accuracy: 0.9480 - val_loss: 2.8256 - val_accuracy: 0.5787 - 36s/epoch - 70ms/step\n",
      "Epoch 60/70\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0852 - accuracy: 0.9553 - val_loss: 2.9322 - val_accuracy: 0.5778 - 35s/epoch - 68ms/step\n",
      "Epoch 61/70\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.59517\n",
      "518/518 - 36s - loss: 0.0882 - accuracy: 0.9526 - val_loss: 2.7717 - val_accuracy: 0.5778 - 36s/epoch - 69ms/step\n",
      "Epoch 62/70\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.59517\n",
      "518/518 - 38s - loss: 0.0857 - accuracy: 0.9543 - val_loss: 3.0792 - val_accuracy: 0.5826 - 38s/epoch - 74ms/step\n",
      "Epoch 63/70\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.59517\n",
      "518/518 - 36s - loss: 0.0863 - accuracy: 0.9555 - val_loss: 3.0143 - val_accuracy: 0.5807 - 36s/epoch - 70ms/step\n",
      "Epoch 64/70\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0909 - accuracy: 0.9500 - val_loss: 2.7149 - val_accuracy: 0.5884 - 35s/epoch - 68ms/step\n",
      "Epoch 65/70\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0837 - accuracy: 0.9567 - val_loss: 2.6775 - val_accuracy: 0.5807 - 35s/epoch - 68ms/step\n",
      "Epoch 66/70\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0872 - accuracy: 0.9551 - val_loss: 2.8511 - val_accuracy: 0.5720 - 35s/epoch - 68ms/step\n",
      "Epoch 67/70\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0863 - accuracy: 0.9538 - val_loss: 2.7810 - val_accuracy: 0.5836 - 35s/epoch - 68ms/step\n",
      "Epoch 68/70\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0826 - accuracy: 0.9592 - val_loss: 2.9932 - val_accuracy: 0.5787 - 35s/epoch - 68ms/step\n",
      "Epoch 69/70\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0850 - accuracy: 0.9526 - val_loss: 2.8108 - val_accuracy: 0.5758 - 35s/epoch - 68ms/step\n",
      "Epoch 70/70\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.59517\n",
      "518/518 - 35s - loss: 0.0824 - accuracy: 0.9563 - val_loss: 3.0646 - val_accuracy: 0.5710 - 35s/epoch - 68ms/step\n"
     ]
    }
   ],
   "source": [
    "model1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Implementing model checkpoins to save the best metric and do not lose it on training.\n",
    "checkpoint1 = ModelCheckpoint(\"best_model1.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1, save_weights_only=False)\n",
    "\n",
    "history = model1.fit(X_train, y_train, epochs=70,\n",
    "                     batch_size=8,\n",
    "                     verbose=2,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     callbacks=[checkpoint1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-13 22:03:18.976224: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 22:03:18.977470: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 22:03:18.978432: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n",
      "2023-05-13 22:03:19.122857: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis' with dtype int32 and shape [1]\n",
      "\t [[{{node gradients/ReverseV2_grad/ReverseV2/ReverseV2/axis}}]]\n",
      "2023-05-13 22:03:19.200183: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_2_grad/concat/split_2/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_2_grad/concat/split_2/split_dim}}]]\n",
      "2023-05-13 22:03:19.202347: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_grad/concat/split/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_grad/concat/split/split_dim}}]]\n",
      "2023-05-13 22:03:19.204057: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'gradients/split_1_grad/concat/split_1/split_dim' with dtype int32\n",
      "\t [[{{node gradients/split_1_grad/concat/split_1/split_dim}}]]\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(max_words, 15, input_length=max_len))\n",
    "model2.add(layers.Bidirectional(layers.LSTM(10,dropout=0.6)))\n",
    "model2.add(layers.Dense(2, activation='sigmoid'))\n",
    "model2.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4915 - accuracy: 0.7515\n",
      "Epoch 1: val_accuracy improved from -inf to 0.55652, saving model to best_model2.hdf5\n",
      "130/130 [==============================] - 6s 45ms/step - loss: 0.4913 - accuracy: 0.7516 - val_loss: 0.7954 - val_accuracy: 0.5565\n",
      "Epoch 2/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.4729 - accuracy: 0.7629\n",
      "Epoch 2: val_accuracy improved from 0.55652 to 0.57101, saving model to best_model2.hdf5\n",
      "130/130 [==============================] - 6s 46ms/step - loss: 0.4729 - accuracy: 0.7629 - val_loss: 0.8135 - val_accuracy: 0.5710\n",
      "Epoch 3/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.7849\n",
      "Epoch 3: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 45ms/step - loss: 0.4524 - accuracy: 0.7849 - val_loss: 0.8381 - val_accuracy: 0.5401\n",
      "Epoch 4/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.4475 - accuracy: 0.7840\n",
      "Epoch 4: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 49ms/step - loss: 0.4475 - accuracy: 0.7840 - val_loss: 0.8340 - val_accuracy: 0.5556\n",
      "Epoch 5/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.7861\n",
      "Epoch 5: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 49ms/step - loss: 0.4440 - accuracy: 0.7859 - val_loss: 0.8683 - val_accuracy: 0.5488\n",
      "Epoch 6/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.4330 - accuracy: 0.7917\n",
      "Epoch 6: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 48ms/step - loss: 0.4330 - accuracy: 0.7917 - val_loss: 0.8324 - val_accuracy: 0.5507\n",
      "Epoch 7/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4265 - accuracy: 0.7914\n",
      "Epoch 7: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.4272 - accuracy: 0.7907 - val_loss: 0.8512 - val_accuracy: 0.5527\n",
      "Epoch 8/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4157 - accuracy: 0.8028\n",
      "Epoch 8: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.4156 - accuracy: 0.8026 - val_loss: 0.8499 - val_accuracy: 0.5546\n",
      "Epoch 9/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4080 - accuracy: 0.8098\n",
      "Epoch 9: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.4083 - accuracy: 0.8098 - val_loss: 0.8815 - val_accuracy: 0.5594\n",
      "Epoch 10/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.4067 - accuracy: 0.8057\n",
      "Epoch 10: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.4079 - accuracy: 0.8052 - val_loss: 0.8726 - val_accuracy: 0.5594\n",
      "Epoch 11/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8123\n",
      "Epoch 11: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3985 - accuracy: 0.8125 - val_loss: 0.9041 - val_accuracy: 0.5556\n",
      "Epoch 12/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3837 - accuracy: 0.8156\n",
      "Epoch 12: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3837 - accuracy: 0.8156 - val_loss: 0.9240 - val_accuracy: 0.5585\n",
      "Epoch 13/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8190\n",
      "Epoch 13: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3865 - accuracy: 0.8190 - val_loss: 0.9489 - val_accuracy: 0.5401\n",
      "Epoch 14/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8146\n",
      "Epoch 14: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3875 - accuracy: 0.8146 - val_loss: 0.9179 - val_accuracy: 0.5507\n",
      "Epoch 15/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3736 - accuracy: 0.8227\n",
      "Epoch 15: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3729 - accuracy: 0.8231 - val_loss: 0.9441 - val_accuracy: 0.5662\n",
      "Epoch 16/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8207\n",
      "Epoch 16: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3726 - accuracy: 0.8209 - val_loss: 0.9255 - val_accuracy: 0.5710\n",
      "Epoch 17/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3642 - accuracy: 0.8340\n",
      "Epoch 17: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3642 - accuracy: 0.8340 - val_loss: 0.8930 - val_accuracy: 0.5681\n",
      "Epoch 18/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8275\n",
      "Epoch 18: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3637 - accuracy: 0.8275 - val_loss: 0.9601 - val_accuracy: 0.5604\n",
      "Epoch 19/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8316\n",
      "Epoch 19: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3603 - accuracy: 0.8318 - val_loss: 0.9349 - val_accuracy: 0.5652\n",
      "Epoch 20/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3605 - accuracy: 0.8277\n",
      "Epoch 20: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3605 - accuracy: 0.8277 - val_loss: 0.9350 - val_accuracy: 0.5681\n",
      "Epoch 21/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3489 - accuracy: 0.8353\n",
      "Epoch 21: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3487 - accuracy: 0.8352 - val_loss: 0.9250 - val_accuracy: 0.5614\n",
      "Epoch 22/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3506 - accuracy: 0.8378\n",
      "Epoch 22: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3506 - accuracy: 0.8378 - val_loss: 0.9924 - val_accuracy: 0.5623\n",
      "Epoch 23/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3339 - accuracy: 0.8450\n",
      "Epoch 23: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 50ms/step - loss: 0.3339 - accuracy: 0.8451 - val_loss: 1.0129 - val_accuracy: 0.5546\n",
      "Epoch 24/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3484 - accuracy: 0.8323\n",
      "Epoch 24: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3484 - accuracy: 0.8323 - val_loss: 0.9517 - val_accuracy: 0.5604\n",
      "Epoch 25/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3386 - accuracy: 0.8391\n",
      "Epoch 25: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 46ms/step - loss: 0.3386 - accuracy: 0.8391 - val_loss: 1.0012 - val_accuracy: 0.5633\n",
      "Epoch 26/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3311 - accuracy: 0.8493\n",
      "Epoch 26: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 48ms/step - loss: 0.3325 - accuracy: 0.8490 - val_loss: 0.9594 - val_accuracy: 0.5507\n",
      "Epoch 27/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3348 - accuracy: 0.8465\n",
      "Epoch 27: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3348 - accuracy: 0.8465 - val_loss: 1.0339 - val_accuracy: 0.5565\n",
      "Epoch 28/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3412 - accuracy: 0.8389\n",
      "Epoch 28: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3411 - accuracy: 0.8391 - val_loss: 1.0025 - val_accuracy: 0.5633\n",
      "Epoch 29/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3234 - accuracy: 0.8513\n",
      "Epoch 29: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.3242 - accuracy: 0.8507 - val_loss: 0.9833 - val_accuracy: 0.5604\n",
      "Epoch 30/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3279 - accuracy: 0.8498\n",
      "Epoch 30: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3279 - accuracy: 0.8499 - val_loss: 0.9734 - val_accuracy: 0.5691\n",
      "Epoch 31/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3281 - accuracy: 0.8445\n",
      "Epoch 31: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.3275 - accuracy: 0.8449 - val_loss: 1.0084 - val_accuracy: 0.5662\n",
      "Epoch 32/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3187 - accuracy: 0.8513\n",
      "Epoch 32: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.3186 - accuracy: 0.8511 - val_loss: 1.0317 - val_accuracy: 0.5671\n",
      "Epoch 33/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3171 - accuracy: 0.8519\n",
      "Epoch 33: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3171 - accuracy: 0.8519 - val_loss: 1.0782 - val_accuracy: 0.5662\n",
      "Epoch 34/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3159 - accuracy: 0.8522\n",
      "Epoch 34: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3157 - accuracy: 0.8521 - val_loss: 1.0281 - val_accuracy: 0.5700\n",
      "Epoch 35/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3236 - accuracy: 0.8500\n",
      "Epoch 35: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3234 - accuracy: 0.8502 - val_loss: 1.0404 - val_accuracy: 0.5671\n",
      "Epoch 36/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8503\n",
      "Epoch 36: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3257 - accuracy: 0.8504 - val_loss: 1.0686 - val_accuracy: 0.5614\n",
      "Epoch 37/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3147 - accuracy: 0.8525\n",
      "Epoch 37: val_accuracy did not improve from 0.57101\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3142 - accuracy: 0.8528 - val_loss: 1.0618 - val_accuracy: 0.5604\n",
      "Epoch 38/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3477 - accuracy: 0.8530\n",
      "Epoch 38: val_accuracy improved from 0.57101 to 0.57198, saving model to best_model2.hdf5\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3477 - accuracy: 0.8531 - val_loss: 1.0115 - val_accuracy: 0.5720\n",
      "Epoch 39/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3178 - accuracy: 0.8510\n",
      "Epoch 39: val_accuracy did not improve from 0.57198\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3195 - accuracy: 0.8499 - val_loss: 1.0283 - val_accuracy: 0.5710\n",
      "Epoch 40/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.8451\n",
      "Epoch 40: val_accuracy improved from 0.57198 to 0.57391, saving model to best_model2.hdf5\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3189 - accuracy: 0.8451 - val_loss: 1.0127 - val_accuracy: 0.5739\n",
      "Epoch 41/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3020 - accuracy: 0.8619\n",
      "Epoch 41: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3030 - accuracy: 0.8613 - val_loss: 1.0129 - val_accuracy: 0.5643\n",
      "Epoch 42/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8566\n",
      "Epoch 42: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3021 - accuracy: 0.8565 - val_loss: 0.9940 - val_accuracy: 0.5594\n",
      "Epoch 43/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2970 - accuracy: 0.8544\n",
      "Epoch 43: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 42ms/step - loss: 0.2976 - accuracy: 0.8540 - val_loss: 1.0473 - val_accuracy: 0.5720\n",
      "Epoch 44/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3008 - accuracy: 0.8571\n",
      "Epoch 44: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.3009 - accuracy: 0.8569 - val_loss: 1.0331 - val_accuracy: 0.5700\n",
      "Epoch 45/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.2969 - accuracy: 0.8610\n",
      "Epoch 45: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.2969 - accuracy: 0.8610 - val_loss: 1.0928 - val_accuracy: 0.5643\n",
      "Epoch 46/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.8615\n",
      "Epoch 46: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 44ms/step - loss: 0.2981 - accuracy: 0.8615 - val_loss: 1.0382 - val_accuracy: 0.5691\n",
      "Epoch 47/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2973 - accuracy: 0.8585\n",
      "Epoch 47: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 46ms/step - loss: 0.2972 - accuracy: 0.8584 - val_loss: 1.0952 - val_accuracy: 0.5652\n",
      "Epoch 48/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.8675\n",
      "Epoch 48: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2880 - accuracy: 0.8676 - val_loss: 1.0564 - val_accuracy: 0.5671\n",
      "Epoch 49/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2927 - accuracy: 0.8706\n",
      "Epoch 49: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2923 - accuracy: 0.8710 - val_loss: 1.1194 - val_accuracy: 0.5594\n",
      "Epoch 50/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.8670\n",
      "Epoch 50: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2884 - accuracy: 0.8673 - val_loss: 1.1181 - val_accuracy: 0.5614\n",
      "Epoch 51/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2898 - accuracy: 0.8619\n",
      "Epoch 51: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2906 - accuracy: 0.8615 - val_loss: 1.0459 - val_accuracy: 0.5565\n",
      "Epoch 52/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.6538 - accuracy: 0.7679\n",
      "Epoch 52: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 45ms/step - loss: 0.6532 - accuracy: 0.7678 - val_loss: 1.0529 - val_accuracy: 0.5662\n",
      "Epoch 53/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3893 - accuracy: 0.8294\n",
      "Epoch 53: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 47ms/step - loss: 0.3893 - accuracy: 0.8294 - val_loss: 1.7050 - val_accuracy: 0.5266\n",
      "Epoch 54/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3621 - accuracy: 0.8321\n",
      "Epoch 54: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 48ms/step - loss: 0.3618 - accuracy: 0.8323 - val_loss: 1.3996 - val_accuracy: 0.5353\n",
      "Epoch 55/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.3224 - accuracy: 0.8490\n",
      "Epoch 55: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 45ms/step - loss: 0.3224 - accuracy: 0.8490 - val_loss: 1.0714 - val_accuracy: 0.5565\n",
      "Epoch 56/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.2979 - accuracy: 0.8639\n",
      "Epoch 56: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 45ms/step - loss: 0.2979 - accuracy: 0.8639 - val_loss: 1.0564 - val_accuracy: 0.5643\n",
      "Epoch 57/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.2892 - accuracy: 0.8635\n",
      "Epoch 57: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2892 - accuracy: 0.8635 - val_loss: 1.0656 - val_accuracy: 0.5671\n",
      "Epoch 58/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3012 - accuracy: 0.8607\n",
      "Epoch 58: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3013 - accuracy: 0.8606 - val_loss: 1.0753 - val_accuracy: 0.5643\n",
      "Epoch 59/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2854 - accuracy: 0.8680\n",
      "Epoch 59: val_accuracy did not improve from 0.57391\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2853 - accuracy: 0.8681 - val_loss: 1.0906 - val_accuracy: 0.5662\n",
      "Epoch 60/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3037 - accuracy: 0.8619\n",
      "Epoch 60: val_accuracy improved from 0.57391 to 0.57778, saving model to best_model2.hdf5\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3051 - accuracy: 0.8615 - val_loss: 1.3945 - val_accuracy: 0.5778\n",
      "Epoch 61/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.3239 - accuracy: 0.8580\n",
      "Epoch 61: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.3236 - accuracy: 0.8584 - val_loss: 1.0489 - val_accuracy: 0.5662\n",
      "Epoch 62/70\n",
      "130/130 [==============================] - ETA: 0s - loss: 0.2837 - accuracy: 0.8739\n",
      "Epoch 62: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2837 - accuracy: 0.8739 - val_loss: 1.1020 - val_accuracy: 0.5681\n",
      "Epoch 63/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2800 - accuracy: 0.8704\n",
      "Epoch 63: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2802 - accuracy: 0.8702 - val_loss: 1.0642 - val_accuracy: 0.5585\n",
      "Epoch 64/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2786 - accuracy: 0.8709\n",
      "Epoch 64: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2790 - accuracy: 0.8710 - val_loss: 1.1034 - val_accuracy: 0.5614\n",
      "Epoch 65/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2794 - accuracy: 0.8781\n",
      "Epoch 65: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2795 - accuracy: 0.8782 - val_loss: 1.0993 - val_accuracy: 0.5623\n",
      "Epoch 66/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2703 - accuracy: 0.8709\n",
      "Epoch 66: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2707 - accuracy: 0.8705 - val_loss: 1.1204 - val_accuracy: 0.5671\n",
      "Epoch 67/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.8755\n",
      "Epoch 67: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2683 - accuracy: 0.8751 - val_loss: 1.1424 - val_accuracy: 0.5643\n",
      "Epoch 68/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2735 - accuracy: 0.8743\n",
      "Epoch 68: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2746 - accuracy: 0.8739 - val_loss: 1.1235 - val_accuracy: 0.5527\n",
      "Epoch 69/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2670 - accuracy: 0.8781\n",
      "Epoch 69: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2679 - accuracy: 0.8777 - val_loss: 1.1110 - val_accuracy: 0.5536\n",
      "Epoch 70/70\n",
      "129/130 [============================>.] - ETA: 0s - loss: 0.2740 - accuracy: 0.8745\n",
      "Epoch 70: val_accuracy did not improve from 0.57778\n",
      "130/130 [==============================] - 6s 43ms/step - loss: 0.2735 - accuracy: 0.8748 - val_loss: 1.1337 - val_accuracy: 0.5643\n"
     ]
    }
   ],
   "source": [
    "checkpoint2 = ModelCheckpoint(\"best_model2.hdf5\", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)\n",
    "history = model2.fit(X_train, y_train, epochs=70,validation_data=(X_test, y_test),callbacks=[checkpoint2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model(\"best_model1.hdf5\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, test_acc = best_model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Model accuracy: ',test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}